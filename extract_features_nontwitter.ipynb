{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Features from non-Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Author: Lauren Thomas\n",
    "#### Created: 16/07/2021\n",
    "#### Last updated: 18/07/2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### File description: This file extracts features for the ML model from the non-Twitter data collected and pre-processed in clean_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ltswe\\anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\ltswe\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.GK7GX5KEQ4F6UYO3P26ULGBQYHGQO7J4.gfortran-win_amd64.dll\n",
      "C:\\Users\\ltswe\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.QVLO2T66WEPI7JZ63PS3HMOHFEY472BC.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from os import sep\n",
    "from patsy import dmatrices\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = f\"C:{sep}Users{sep}ltswe{sep}Dropbox{sep}Oxford{sep}Thesis\"\n",
    "data_dir = \"D:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_non_nyc(raw_df):\n",
    "    return raw_df[(raw_df['STATE'] == 'New York') & ((raw_df['COUNTY'] == 'Bronx County') | \n",
    "                                              (raw_df['COUNTY'] == 'New York County') |\n",
    "                                              (raw_df['COUNTY'] == 'Kings County') |\n",
    "                                              (raw_df['COUNTY'] == 'Queens County') |\n",
    "                                              (raw_df['COUNTY'] == 'Richmond County'))].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring in the socioeconomic features & total pop counts from 2010-2014\n",
    "\n",
    "# Import 2010-2014 data estimates \n",
    "est_1014_raw = pd.read_csv(f\"{cwd}{sep}data{sep}2010_2014_estimates.csv\", encoding = 'latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of data outside NYC \n",
    "est_1014 = drop_non_nyc(est_1014_raw)\n",
    "\n",
    "# Rename var to total pop\n",
    "est_1014['total_pop_1014'] = est_1014['ABA1E001']\n",
    "\n",
    "# Fix fips code\n",
    "# Make a more understandable fips code (11 digits instead of 13)\n",
    "def gen_fips_from_gisjoin(x):\n",
    "    ''' x = GISJOIN code '''\n",
    "    # x[4:7] is county code, x[8:14] = census tract code ('36' is always state code)\n",
    "    return \"36\" + x[4:7] + x[8:14]\n",
    "\n",
    "est_1014['LocationCT'] = est_1014['GISJOIN'].apply(lambda x: gen_fips_from_gisjoin(x))\n",
    "\n",
    "# Keep only relevant var (fips code and total pop)\n",
    "est_1014 = est_1014[['LocationCT', 'total_pop_1014']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring in data from 2006_2010_estimates_p2 to calc unemployment\n",
    "est_0610_raw = pd.read_csv(f\"{cwd}{sep}data{sep}2006_2010_estimates_p2.csv\", encoding = 'latin-1')\n",
    "\n",
    "# Get rid of data outside NYC \n",
    "est_0610 = drop_non_nyc(est_0610_raw)\n",
    "\n",
    "# Find unemployed\n",
    "est_0610['%_unemployed_0610'] = (est_0610['J6QE008'] + est_0610['J6QE015'] + \n",
    "                           est_0610['J6QE022'] + est_0610['J6QE029'] + est_0610['J6QE036'] \n",
    "                           + est_0610['J6QE043'] + est_0610['J6QE050'] + est_0610['J6QE057']\n",
    "                           + est_0610['J6QE064'] + est_0610['J6QE076'] + est_0610['J6QE071']\n",
    "                           + est_0610['J6QE081'] + est_0610['J6QE086'] + est_0610['J6QE094']\n",
    "                           + est_0610['J6QE101'] + est_0610['J6QE108'] + est_0610['J6QE115']\n",
    "                           + est_0610['J6QE122'] + est_0610['J6QE129']\n",
    "                           + est_0610['J6QE136'] + est_0610['J6QE143'] + est_0610['J6QE150'] \n",
    "                           + est_0610['J6QE157'] + est_0610['J6QE162'] + est_0610['J6QE167']\n",
    "                           + est_0610['J6QE172'])/(est_0610['J6QE004'] + est_0610['J6QE011'] + \n",
    "                           est_0610['J6QE018'] + est_0610['J6QE025'] + est_0610['J6QE032'] \n",
    "                           + est_0610['J6QE039'] + est_0610['J6QE046'] + est_0610['J6QE053']\n",
    "                           + est_0610['J6QE060'] + est_0610['J6QE067'] + est_0610['J6QE074']\n",
    "                           + est_0610['J6QE079'] + est_0610['J6QE084'] + est_0610['J6QE090']\n",
    "                           + est_0610['J6QE097'] + est_0610['J6QE104'] + est_0610['J6QE111']\n",
    "                           + est_0610['J6QE118'] + est_0610['J6QE125']\n",
    "                           + est_0610['J6QE132'] + est_0610['J6QE139'] + est_0610['J6QE146'] \n",
    "                           + est_0610['J6QE153'] + est_0610['J6QE160'] + est_0610['J6QE165']\n",
    "                           + est_0610['J6QE170'])\n",
    "\n",
    "est_0610['LocationCT'] = est_0610['GISJOIN'].apply(lambda x: gen_fips_from_gisjoin(x))\n",
    "\n",
    "# Keep only relevant var (fips code and total pop)\n",
    "est_0610 = est_0610[['LocationCT', '%_unemployed_0610']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring in total pop data\n",
    "pop_0610_raw = pd.read_csv(f\"{cwd}{sep}data{sep}2006_2010_pop.csv\", encoding = 'latin-1')\n",
    "pop_0610 = drop_non_nyc(pop_0610_raw)\n",
    "pop_0610['total_pop_0610'] = pop_0610['JMAE001']\n",
    "\n",
    "pop_0610['LocationCT'] = pop_0610['GISJOIN'].apply(lambda x: gen_fips_from_gisjoin(x))\n",
    "\n",
    "\n",
    "pop_0610 = pop_0610[['LocationCT', 'total_pop_0610']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpickle socioeconomic data\n",
    "socio_df = pickle.load(open(f'{data_dir}{sep}pickle{sep}all_socioeconomic.pickle', 'rb'))\n",
    "socio_df['LocationCT'] = socio_df['fips_code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine socioecon df\n",
    "socio_merged = socio_df.merge(est_1014.merge(est_0610.merge(pop_0610, on='LocationCT'), on = 'LocationCT'), on='LocationCT')\n",
    "\n",
    "# Pickle\n",
    "pickle.dump(socio_merged, open(f'{data_dir}{sep}pickle{sep}socio_merged.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 311 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Unpickle 311 data set\n",
    "# nyc_311 = pickle.load(open(f\"{data_dir}{sep}pickle{sep}nyc_311_gdf.pickle\", 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Replace the various noise categories with just Noise\n",
    "# nyc_311 = nyc_311.replace(to_replace = ['Noise - Street/Sidewalk', 'Noise - Vehicle', 'Noise - Residential',\n",
    "#                                        'Noise - Park', 'Noise - House of Worship'], \n",
    "#                           value=['Noise', 'Noise', 'Noise', 'Noise', 'Noise'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # More detailed info about the relevant complaints \n",
    "# nyc_311[(nyc_311['complaint_type'] == 'Noise') |\n",
    "#                        (nyc_311['complaint_type'] == 'For Hire Vehicle Complaint') |\n",
    "#                        (nyc_311['complaint_type'] == 'Bike Rack Condition')]['descriptor'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 311 data: Collapse # of noise/for hire vehicle/bike rack complaints into year-month-census tract\n",
    "# nyc_311_count = nyc_311[(nyc_311['complaint_type'] == 'Noise') |\n",
    "#                        (nyc_311['complaint_type'] == 'For Hire Vehicle Complaint') |\n",
    "#                        (nyc_311['complaint_type'] == 'Bike Rack Condition')] \\\n",
    "#                         .groupby(['year', 'month', 'LocationCT', 'complaint_type']).count()['unique_key']\\\n",
    "#                         .reset_index().rename(columns={'unique_key': 'count'})\n",
    "# # Pickle\n",
    "# pickle.dump(nyc_311_count, open(f'{data_dir}{sep}pickle{sep}nyc_311_count.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_311_count = pickle.load(open(f'{data_dir}{sep}pickle{sep}nyc_311_count.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_311_1113 = nyc_311_count[(nyc_311_count['year'] == '2011')| (nyc_311_count['year'] == '2012') |\n",
    "                            (nyc_311_count['year'] == '2013')].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge in pop data to calc pc\n",
    "# Merge in population for 2006-2010 & 10-14 from socio_df\n",
    "pop = socio_merged[['LocationCT', 'total_pop_1014']]\n",
    "nyc_311_1113 = nyc_311_1113.merge(pop, on = 'LocationCT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_311_1113['complaints_pc'] = nyc_311_1113['count']/nyc_311_1113['total_pop_1014']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create year-month var\n",
    "nyc_311_1113['ym'] = nyc_311_1113.year.str.cat(nyc_311_1113.month.apply(lambda x: (\"0\"+x)[-2:]),sep='-')\n",
    "\n",
    "# Make year-month into numbers and let it equal number of months since Jan 2006 (starting at 60)\n",
    "nyc_311_1113['ym_num'] = nyc_311_1113['ym'].apply(lambda x: (int(x[0:4])-2006)*12+int(x[-2:])-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of all ym_num and tracts. Recall ym_num starts at 60 and goes till 96 (3 years)\n",
    "ym_num_list_311 = [i for i in range(60,96)]\n",
    "tract_list_311 = nyc_311_1113['LocationCT'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Any missing ym-tract-levels not in dataframe should be added in with a '0' count\n",
    "# Create a function to do this with the given df\n",
    "def replace_missing(df, tract_list, ym_num_list, crime_df = True):\n",
    "    missing_tracts, missing_ym_num, missing_levels = [],[],[] \n",
    "    # Level = futher thing to separate by, e.g, level of crime or type of complaint\n",
    "    if crime_df == True:\n",
    "        level_list = ['FELONY', 'MISDEMEANOR', 'VIOLATION']\n",
    "        level_str = 'level'\n",
    "    else:\n",
    "        level_list = ['Noise', 'For Hire Vehicle Complaint', 'Bike Rack Condition']\n",
    "        level_str = 'complaint_type'\n",
    "    # Convert all the currently existing tracts/ym/level combos into a dictionary\n",
    "    existing_dict = df.groupby('LocationCT')[['ym_num', level_str, 'count']].apply(lambda x: x.set_index(['ym_num', level_str])\\\n",
    "                                                        .to_dict()).to_dict()\n",
    "    \n",
    "    # First, go through and find the missing ym-tract-level combos\n",
    "    for tract in tract_list: \n",
    "        for ym_num in ym_num_list:\n",
    "            for level in level_list:\n",
    "                # If the following returns a Key Error, then it's not in the existing dict (missing)\n",
    "                try: \n",
    "                    existing_dict[tract]['count'][ym_num, level]\n",
    "                except KeyError:\n",
    "                    missing_tracts.append(tract)\n",
    "                    missing_ym_num.append(ym_num)\n",
    "                    missing_levels.append(level)\n",
    "    \n",
    "    # Next, create a list = len of tracts/ym_num/levels. crime or complaint type pc is 0 for all of these\n",
    "    pc_list = [0.0 for i in range(len(missing_tracts))]\n",
    "    \n",
    "    # Now, create a dataframe tht we will append onto original df\n",
    "    new_df = pd.DataFrame()\n",
    "    new_df['LocationCT'] = missing_tracts\n",
    "    new_df[level_str] = missing_levels\n",
    "    if crime_df == True:\n",
    "        new_df['crime_pc'] = pc_list\n",
    "    else:\n",
    "        new_df['complaints_pc'] = pc_list\n",
    "    new_df['ym_num'] = missing_ym_num\n",
    "    \n",
    "    full_df = df.append(new_df)\n",
    "    \n",
    "    return full_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_311_1113_full = replace_missing(nyc_311_1113, tract_list_311, ym_num_list_311, crime_df=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use nyc_311_1113_full to find the trend line\n",
    "# Create a dict where key = LocationCT, value = trend (from linear regression) for each complaint type.\n",
    "# Create a function that when given a df & LocationCT, calculates the line of best fit and puts it in the dict\n",
    "def calc_trend(df, LocationCT, trend_dict, complaint_type):\n",
    "    # Filter based on census tract\n",
    "    # Calc line of best fit\n",
    "    y,X = dmatrices('complaints_pc ~ ym_num', data=df[(df['complaint_type'] == complaint_type) & (df['LocationCT'] == LocationCT)], return_type='dataframe')\n",
    "    model = sm.OLS(y,X)\n",
    "    # Fit model\n",
    "    res = model.fit()\n",
    "    # Put OLS line slope into trend dict\n",
    "    trend_dict[LocationCT] = res.params[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calc trend for each type of complaint & location\n",
    "def calc_trend_complaint(complaint_type, trend_dict):\n",
    "    for tract in tract_list_311:\n",
    "        calc_trend(nyc_311_1113_full, tract, trend_dict, complaint_type)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_trend_dict = dict()\n",
    "fhv_trend_dict = dict()\n",
    "bike_trend_dict = dict()\n",
    "\n",
    "calc_trend_complaint('Noise', noise_trend_dict)\n",
    "calc_trend_complaint('For Hire Vehicle Complaint', fhv_trend_dict)\n",
    "calc_trend_complaint('Bike Rack Condition', bike_trend_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn features into dataframe and merge\n",
    "df1 = pd.DataFrame.from_dict(noise_trend_dict, \n",
    "                             orient='index').reset_index().rename(columns={'index': 'LocationCT', 0:'noise_1113_trend'})\n",
    "df2 = pd.DataFrame.from_dict(fhv_trend_dict, \n",
    "                             orient='index').reset_index().rename(columns={'index': 'LocationCT', 0:'fhv_1113_trend'})\n",
    "df3 = pd.DataFrame.from_dict(bike_trend_dict, \n",
    "                             orient='index').reset_index().rename(columns={'index': 'LocationCT', 0:'bike_1113_trend'})\n",
    "\n",
    "features_311 = df1.merge(df2.merge(df3, on='LocationCT'), on='LocationCT')\n",
    "\n",
    "# Pickle features\n",
    "pickle.dump(features_311, open(f'{data_dir}{sep}pickle{sep}features_311.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Unpickle nyc crime\n",
    "# nyc_crime = pickle.load(open(f'{data_dir}{sep}pickle{sep}nyc_crime_gdf.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Drop the rows that contain no values for pd_desc or class (or 4 that contain no value for 'completed')\n",
    "# nyc_crime = nyc_crime.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Features\n",
    "# # Collapse crime into year-month-census tracts-level \n",
    "# nyc_crime_count = nyc_crime.groupby(['year', 'month', 'LocationCT', 'level']).count()['cmplnt_num']\\\n",
    "#                     .reset_index().rename(columns={'cmplnt_num': 'count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(nyc_crime_count, open(f'{data_dir}{sep}pickle{sep}nyc_crime_count.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_crime_count = pickle.load(open(f'{data_dir}{sep}pickle{sep}nyc_crime_count.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge in population for 2006-2010 & 10-14 from socio_df\n",
    "pop = socio_merged[['LocationCT', 'total_pop_1014', 'total_pop_0610']]\n",
    "nyc_crime_count = nyc_crime_count.merge(pop, on = 'LocationCT')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2006-2010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average crime pc 2006-2010\n",
    "nyc_crime_0610 = nyc_crime_count[(nyc_crime_count['year'] == \"2006\") |(nyc_crime_count['year'] == \"2007\") | (nyc_crime_count['year'] == \"2008\") | (nyc_crime_count['year'] == \"2009\")\n",
    "                                | (nyc_crime_count['year'] == \"2010\")].reset_index(drop=True)\n",
    "nyc_crime_0610['crime_pc'] = nyc_crime_0610['count']/nyc_crime_0610['total_pop_0610']\n",
    "\n",
    "# Create year-month var\n",
    "nyc_crime_0610['ym'] = nyc_crime_0610.year.str.cat(nyc_crime_0610.month.apply(lambda x: (\"0\"+x)[-2:]),sep='-')\n",
    "\n",
    "# Make year-month into numbers and let it equal number of months since Jan 2006 (starting at 0)\n",
    "nyc_crime_0610['ym_num'] = nyc_crime_0610['ym'].apply(lambda x: (int(x[0:4])-2006)*12+int(x[-2:])-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of all ym_num and tracts\n",
    "ym_num_list = [i for i in range(60)]\n",
    "tract_list = nyc_crime_0610['LocationCT'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use replace missing function from 311 section to add in 0s for the \n",
    "nyc_crime_0610_full = replace_missing(nyc_crime_0610, tract_list, ym_num_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collapse to get sum of the various crimes & then average the months/years\n",
    "nyc_crime_0610_tot = nyc_crime_0610_full.groupby(['ym_num', 'LocationCT']).sum()['crime_pc'].reset_index()\n",
    "nyc_crime_0610_avg = nyc_crime_0610_tot.groupby(['LocationCT']).mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use nyc_crime_tot to find the trend line\n",
    "# Create a dict where key = LocationCT, value = trend (from linear regression).\n",
    "# Create a function that when given a df & LocationCT, calculates the line of best fit and puts it in the dict\n",
    "def calc_trend(df, LocationCT, trend_dict):\n",
    "    # Filter based on census tract\n",
    "    # Calc line of best fit\n",
    "    y,X = dmatrices('crime_pc ~ ym_num', data=df[df['LocationCT'] == LocationCT], return_type='dataframe')\n",
    "    model = sm.OLS(y,X)\n",
    "    # Fit model\n",
    "    res = model.fit()\n",
    "    # Put OLS line slope into trend dict\n",
    "    trend_dict[LocationCT] = res.params[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "crime_0610_trend_dict = dict()\n",
    "\n",
    "# Now, run through all the census tracts to calc line of best fit\n",
    "for tract in tract_list:\n",
    "    calc_trend(nyc_crime_0610_tot, tract, crime_0610_trend_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate into felonies only from nyc_crime_0710. \n",
    "nyc_fel_0610 = nyc_crime_0610_full[nyc_crime_0610_full['level'] == 'FELONY']\n",
    "# Get average\n",
    "nyc_fel_0610_avg = nyc_fel_0610.groupby(['LocationCT']).mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the trend line using nyc_fel_0610\n",
    "fel_0610_trend_dict = dict()\n",
    "for tract in tract_list:\n",
    "    calc_trend(nyc_fel_0610, tract, fel_0610_trend_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2011-2013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average crime pc 2011-2013\n",
    "nyc_crime_1113 = nyc_crime_count[(nyc_crime_count['year'] == \"2011\") |\n",
    "                                 (nyc_crime_count['year'] == \"2012\") | \n",
    "                                 (nyc_crime_count['year'] == \"2013\")].reset_index(drop=True)\n",
    "\n",
    "nyc_crime_1113['crime_pc'] = nyc_crime_1113['count']/nyc_crime_1113['total_pop_1014']\n",
    "\n",
    "# Create year-month var\n",
    "nyc_crime_1113['ym'] = nyc_crime_1113.year.str.cat(nyc_crime_1113.month.apply(lambda x: (\"0\"+x)[-2:]),sep='-')\n",
    "\n",
    "# Make year-month into numbers and let it equal number of months since Jan 2006 (starting at 60 for Jan 2011)\n",
    "nyc_crime_1113['ym_num'] = nyc_crime_1113['ym'].apply(lambda x: (int(x[0:4])-2006)*12+int(x[-2:])-1)\n",
    "\n",
    "# Create list of all ym_num and tracts\n",
    "ym_num_1113_list = [i for i in range(60,96)]\n",
    "tract_list = list(nyc_crime_1113['LocationCT'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace missing using above function\n",
    "nyc_crime_1113_full = replace_missing(nyc_crime_1113, tract_list, ym_num_1113_list)\n",
    "\n",
    "# Collapse to get sum of the various crimes & then average the months/years\n",
    "nyc_crime_1113_tot = nyc_crime_1113_full.groupby(['ym_num', 'LocationCT']).sum()['crime_pc'].reset_index()\n",
    "nyc_crime_1113_avg = nyc_crime_1113_tot.groupby(['LocationCT']).mean().reset_index()[['LocationCT', 'crime_pc']]\n",
    "\n",
    "# Find trend lines for crimes in 2011-2013\n",
    "crime_1113_trend_dict = dict()\n",
    "\n",
    "# Now, run through all the census tracts to calc line of best fit\n",
    "for tract in tract_list:\n",
    "    calc_trend(nyc_crime_1113_tot, tract, crime_1113_trend_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate into felonies only from nyc_crime_1113. \n",
    "nyc_fel_1113 = nyc_crime_1113_full[nyc_crime_1113_full['level'] == 'FELONY']\n",
    "# Get average\n",
    "nyc_fel_1113_avg = nyc_fel_1113.groupby(['LocationCT']).mean().reset_index()\n",
    "\n",
    "# Find the trend line using nyc_fel_1113\n",
    "fel_1113_trend_dict = dict()\n",
    "for tract in tract_list:\n",
    "    calc_trend(nyc_fel_1113, tract, fel_1113_trend_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe with LocationCT & all the other relevant crime vars\n",
    "# First, turn the trend lines (crime0610, crime1113, fel0610, fel1113) into dataframes\n",
    "df1 = pd.DataFrame.from_dict(crime_0610_trend_dict, \n",
    "                             orient='index').reset_index().rename(columns={'index': 'LocationCT', 0:'crime_0610_trend'})\n",
    "\n",
    "df2 = pd.DataFrame.from_dict(crime_1113_trend_dict, \n",
    "                             orient='index').reset_index().rename(columns={'index': 'LocationCT', 0:'crime_1113_trend'})\n",
    "\n",
    "df3 = pd.DataFrame.from_dict(fel_0610_trend_dict, \n",
    "                             orient='index').reset_index().rename(columns={'index': 'LocationCT', 0:'fel_0610_trend'})\n",
    "\n",
    "df4 = pd.DataFrame.from_dict(fel_1113_trend_dict, \n",
    "                             orient='index').reset_index().rename(columns={'index': 'LocationCT', 0:'fel_1113_trend'})\n",
    "\n",
    "\n",
    "trend_df = df1.merge(df2.merge(df3.merge(df4, on='LocationCT'), on='LocationCT'), on='LocationCT')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix dataframes for the averages\n",
    "nyc_crime_0610_avg = nyc_crime_0610_avg[['LocationCT', 'crime_pc']].rename(columns={'crime_pc': 'crime_pc_0610'})\n",
    "nyc_crime_1113_avg = nyc_crime_1113_avg[['LocationCT', 'crime_pc']].rename(columns={'crime_pc': 'crime_pc_1113'})\n",
    "\n",
    "nyc_fel_0610_avg = nyc_fel_0610_avg[['LocationCT', 'crime_pc']].rename(columns={'crime_pc': 'fel_pc_0610'})\n",
    "nyc_fel_1113_avg = nyc_fel_1113_avg[['LocationCT', 'crime_pc']].rename(columns={'crime_pc': 'fel_pc_1113'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, merge the various per-capita averages\n",
    "crime_features = trend_df.merge(nyc_crime_0610_avg.merge(nyc_crime_1113_avg.merge(nyc_fel_0610_avg.merge(nyc_fel_1113_avg, \n",
    "                                on='LocationCT'), on='LocationCT'), on='LocationCT'), on='LocationCT')\n",
    "\n",
    "# Pickle crime features\n",
    "pickle.dump(crime_features, open(f'{data_dir}{sep}pickle{sep}crime_features.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HUD Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring in HUD dataset\n",
    "ny_hud = pickle.load(open(f\"{data_dir}{sep}pickle{sep}nyc_hud.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sep county codes and keep only NYC counties (061, 005, 081, 085, 047)\n",
    "ny_hud['county_code'] = ny_hud['fips_code'].apply(lambda x: x[2:5])\n",
    "nyc_hud = ny_hud[(ny_hud['county_code'] == '061') | (ny_hud['county_code'] == '005') | (ny_hud['county_code'] == '081') |\n",
    "                (ny_hud['county_code'] == '085') | (ny_hud['county_code'] == '047')].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total # of commerical addresses, total # of res addresses, total # of vacant addresses (res and comm) ST, total \n",
    "# num of res/comm addresses LT\n",
    "nyc_hud['total_res'] = nyc_hud['AMS_RES']\n",
    "nyc_hud['total_comm'] = nyc_hud['AMS_BUS']\n",
    "nyc_hud['total_vacant_res'] = nyc_hud['RES_VAC']\n",
    "nyc_hud['total_vacant_comm'] = nyc_hud['BUS_VAC']\n",
    "nyc_hud['avg_vac_res'] = nyc_hud['AVG_VAC_R']\n",
    "nyc_hud['avg_vac_comm'] = nyc_hud['AVG_VAC_B']\n",
    "\n",
    "nyc_hud['total_vacant_ST'] = nyc_hud['VAC_3_RES'] + nyc_hud['VAC_3_6_R'] + nyc_hud['VAC_3_BUS'] + nyc_hud['VAC_3_6_B']\n",
    "nyc_hud['total_res_vacant_ST'] = nyc_hud['VAC_3_RES'] + nyc_hud['VAC_3_6_R']\n",
    "nyc_hud['total_comm_vacant_ST'] =nyc_hud['VAC_3_BUS'] + nyc_hud['VAC_3_6_B']\n",
    "\n",
    "nyc_hud['total_res_vacant_LT']  = nyc_hud['VAC_6_12R'] +  nyc_hud['VAC_12_24R'] + nyc_hud['VAC_24_36R'] + nyc_hud['VAC_36_RES']\n",
    "nyc_hud['total_comm_vacant_LT']= nyc_hud['VAC_6_12B'] +  nyc_hud['VAC_12_24B'] + nyc_hud['VAC_24_36B'] + nyc_hud['VAC_36_BUS']\n",
    "nyc_hud['total_vacant_LT'] = nyc_hud['total_res_vacant_LT'] + nyc_hud['total_comm_vacant_LT']\n",
    "\n",
    "nyc_hud_full = nyc_hud[['fips_code', 'year', 'month','total_res', 'total_comm', 'total_vacant_res', 'total_vacant_comm',\n",
    "                       'avg_vac_res', 'avg_vac_comm', 'total_vacant_ST', 'total_res_vacant_ST',\n",
    "                       'total_comm_vacant_ST', 'total_res_vacant_LT', 'total_comm_vacant_LT', 'total_vacant_LT']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gen ym variables\n",
    "# Create year-month var\n",
    "nyc_hud_full['ym'] = nyc_hud_full.year.str.cat(nyc_hud_full.month.apply(lambda x: (\"0\"+x)[-2:]),sep='-')\n",
    "\n",
    "# Make year-month into numbers and let it equal number of months since Jan 2006 (starting at 77 - 2012-06)\n",
    "nyc_hud_full['ym_num'] = nyc_hud_full['ym'].apply(lambda x: (int(x[0:4])-2006)*12+int(x[-2:])-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_hud_tract_list = nyc_hud_full['fips_code'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use nyc_hud_full to find the trend line\n",
    "# Create a dict where key = LocationCT, value = trend (from linear regression).\n",
    "# Create a function that when given a df & LocationCT, calculates the line of best fit and puts it in the dict\n",
    "def calc_trend(df, fips_code, trend_dict, var_str):\n",
    "    # Filter based on census tract\n",
    "    # Calc line of best fit\n",
    "    y,X = dmatrices(f'{var_str} ~ ym_num', data=df[df['fips_code'] == fips_code], return_type='dataframe')\n",
    "    model = sm.OLS(y,X)\n",
    "    # Fit model\n",
    "    res = model.fit()\n",
    "    # Put OLS line slope into trend dict\n",
    "    trend_dict[var_str][fips_code] = res.params[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find trend lines for crimes in 2011-2013\n",
    "nyc_hud_trend_dict = dict()\n",
    "var_str_list = ['total_comm', 'total_res', 'total_res_vacant_ST', 'total_comm_vacant_ST', 'total_res_vacant_LT', 'total_comm_vacant_LT']\n",
    "\n",
    "# Now, run through all the census tracts to calc line of best fit\n",
    "for var_str in var_str_list:\n",
    "    nyc_hud_trend_dict[var_str] = dict()\n",
    "    for tract in nyc_hud_tract_list:\n",
    "        calc_trend(nyc_hud_full, tract, nyc_hud_trend_dict, var_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame.from_dict(nyc_hud_trend_dict['total_comm'], \n",
    "                             orient='index').reset_index().rename(columns={'index': 'LocationCT', 0:'total_comm_trend'})\n",
    "\n",
    "df2 = pd.DataFrame.from_dict(nyc_hud_trend_dict['total_res'], \n",
    "                             orient='index').reset_index().rename(columns={'index': 'LocationCT', 0:'total_res_trend'})\n",
    "\n",
    "df3 = pd.DataFrame.from_dict(nyc_hud_trend_dict['total_res_vacant_ST'], \n",
    "                             orient='index').reset_index().rename(columns={'index': 'LocationCT', 0:'total_res_vacant_ST_trend'})\n",
    "\n",
    "df4 = pd.DataFrame.from_dict(nyc_hud_trend_dict['total_comm_vacant_ST'], \n",
    "                             orient='index').reset_index().rename(columns={'index': 'LocationCT', 0:'total_comm_vacant_ST_trend'})\n",
    "\n",
    "df5 = pd.DataFrame.from_dict(nyc_hud_trend_dict['total_res_vacant_LT'], \n",
    "                             orient='index').reset_index().rename(columns={'index': 'LocationCT', 0:'total_res_vacant_LT_trend'})\n",
    "\n",
    "df6 = pd.DataFrame.from_dict(nyc_hud_trend_dict['total_comm_vacant_LT'], \n",
    "                             orient='index').reset_index().rename(columns={'index': 'LocationCT', 0:'total_comm_vacant_LT_trend'})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "hud_features = df1.merge(df2.merge(df3.merge(df4.merge(df5.merge(df6, on='LocationCT'), on='LocationCT'), on='LocationCT'), on='LocationCT'), on='LocationCT')\n",
    "\n",
    "# Pickle features\n",
    "pickle.dump(hud_features, open(f'{data_dir}{sep}pickle{sep}hud_features.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Merge all features together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpickle relevant dataframes\n",
    "socio_merged = pickle.load(open(f'{data_dir}{sep}pickle{sep}socio_merged.pickle', 'rb'))\n",
    "hud_features = pickle.load(open(f'{data_dir}{sep}pickle{sep}hud_features.pickle', 'rb'))\n",
    "crime_features = pickle.load(open(f'{data_dir}{sep}pickle{sep}crime_features.pickle', 'rb'))\n",
    "features_311 = pickle.load(open(f'{data_dir}{sep}pickle{sep}features_311.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['GISJOIN', 'STATE', 'COUNTY', '%_black_0610', '%_white_0610',\n",
       "       '25plus_pop_0610', '%_bachelors_0610', '%_nonwhite_0610',\n",
       "       '%_hhrent_0610', '%_li_0610', 'med_hh_income_0610', 'med_rent_0610',\n",
       "       'med_home_value_0610', '25plus_pop_1418', '%_bachelors_1418',\n",
       "       'med_hh_income_1418', 'med_rent_1418', 'med_home_value_1418',\n",
       "       'change_rent', 'change_home_value', 'eligible_gentrify', 'hot_market',\n",
       "       'change_college', 'change_hh_inc', 'gentrification', 'fips_code',\n",
       "       'LocationCT', 'total_pop_1014', '%_unemployed_0610', 'total_pop_0610'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "socio_merged.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_twitter_features = socio_merged.merge(hud_features.merge(crime_features.merge(features_311, on='LocationCT'), on = 'LocationCT'), on='LocationCT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(non_twitter_features, open(f'{data_dir}{sep}pickle{sep}non_twitter_features.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
