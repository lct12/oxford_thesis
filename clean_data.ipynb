{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and Clean Data Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "import glob\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import osmium as osm\n",
    "from os import sep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working directory\n",
    "cwd = f\"C:{sep}Users{sep}ltswe{sep}Dropbox{sep}Oxford{sep}Thesis\"\n",
    "# Data directory is kept on flash\n",
    "data_dir = \"D:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of the JSONs that will need to be loaded in \n",
    "# Glob.glob creates a list of all the files that end in .json in the directories/sub-directories of raw_tweets\n",
    "# The rest of the command filters out jsons that end in 00000.json since those represent meta counts and not actual tweets\n",
    "json_list = [j for j in glob.glob(f'{data_dir}{sep}raw_tweets{sep}**{sep}*.json', recursive=True)\n",
    "             if j[-10:] != '00000.json']\n",
    "len(json_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn into pandas dataframe using pd.concat\n",
    "# Commence with blank dataframe\n",
    "raw_df = pd.DataFrame()\n",
    "for j in json_list:\n",
    "    temp_json = json.load(open(j))\n",
    "    temp_df = pd.json_normalize(temp_json['data'])\n",
    "    raw_df = pd.concat((raw_df, temp_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Pickle the raw data \n",
    "raw_tweets_pickle = open(f\"{data_dir}{sep}pickle{sep}raw_tweets_df.pickle\", \"wb\")\n",
    "pickle.dump(raw_df, raw_tweets_pickle)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring in pickled data\n",
    "raw_df = pickle.load(open(f\"{data_dir}{sep}pickle{sep}raw_tweets_df.pickle\", \"rb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring in crime and 311 data, which uses the Socrata API in NYC Open Data\n",
    "# Create a function that uses the Socrata API, which is written in SoQL, a SQL-like language, to query data\n",
    "from sodapy import Socrata\n",
    "\n",
    "def socrata_API_df(source_domain, dataset_id, select_string, where_string, limit=1000):\n",
    "    '''\n",
    "    Inputs: \n",
    "    source_domain: This tells Socrata the source of the dataset you're querying\n",
    "    dataset_id: This is the unique id of the dataset\n",
    "    select_string: This string tells Socrata which variables you are selecting from the dataset\n",
    "    where_string: This string is equivalent to the \"where\" command in SQL\n",
    "    limit = This tells Socrata how many results to query. The default is 1000 b/c Socrata automatically sets it to 1000\n",
    "\n",
    "    Outputs a dataframe with with the queried results\n",
    "    '''\n",
    "    keyFile = open(f'{cwd}{sep}tokens{sep}socrata_apikey.txt', 'r')\n",
    "    token = keyFile.readline() #api token imported from txt file\n",
    "    \n",
    "    client = Socrata(source_domain, token)\n",
    "    # Change timeout var to arbitrarily large # of seconds so it doesn't time out\n",
    "    client.timeout = 50\n",
    "    results = client.get(dataset_id, limit = limit, select = select_string, where = where_string)\n",
    "    df = pd.DataFrame.from_records(results)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to shorten lat/long\n",
    "# Cut lat/long to 4 decimals (10 m)\n",
    "def round_lat_long(df):\n",
    "    df[\"latitude\"] = df[\"latitude\"].apply(lambda x: round(float(x),3))\n",
    "    df[\"longitude\"]= df[\"longitude\"].apply(lambda x: round(float(x),3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull in 311 and Null Data \n",
    "# 2007, 2008, & 2009 are separate; 2010-on are in a single file. \n",
    "# The only thing that changes between 2007-09 is the dataset ID, & the id + where string for 2010-on\n",
    "# so write a function that calls upon the 311 socrata API data\n",
    "# complaint type string -- separated for ease of understanding. Complaint types drawn from literature\n",
    "complaint_type_str = \"complaint_type = 'Noise - Street/Sidewalk' OR complaint_type = 'Noise - Residential' OR complaint_type = 'Noise - Vehicle' OR complaint_type = 'Street Condition' \" \\\n",
    "                    \"OR complaint_type = 'Homeless Encampment' OR complaint_type = 'Drinking' OR complaint_type = 'Noise' \" \\\n",
    "                    \"OR complaint_type = 'Noise - Park' OR complaint_type = 'Noise - House of Worship' OR complaint_type = 'HEATING' \" \\\n",
    "                    \"OR complaint_type = 'GENERAL CONSTRUCTION' OR complaint_type = 'CONSTRUCTION' OR complaint_type = 'Boilers' \" \\\n",
    "                    \"OR complaint_type = 'For Hire Vehicle Complaint' OR complaint_type = 'Bike Rack Condition' OR complaint_type = 'Illegal Parking' \" \\\n",
    "                    \"OR complaint_type = 'Building/Use' OR complaint_type = 'ELECTRIC' OR complaint_type = 'PLUMBING'\"\n",
    "\n",
    "def pull_311(dataset_id, where_string = f'latitude IS NOT NULL AND ({complaint_type_str})'):\n",
    "    return socrata_API_df(source_domain = \"data.cityofnewyork.us\", dataset_id = dataset_id, \\\n",
    "                         select_string = 'unique_key, created_date, complaint_type, date_extract_y(created_date) as year, date_extract_m(created_date) as month, descriptor, latitude, longitude', \\\n",
    "                         where_string = where_string,\n",
    "                         limit = 4000000)\n",
    "\n",
    "# 2007-2013\n",
    "nyc_311_07 = pull_311(\"aiww-p3af\")\n",
    "nyc_311_08 = pull_311('uzcy-9puk')\n",
    "nyc_311_09 = pull_311('3rfa-3xsf')\n",
    "nyc_311_10_13 = pull_311('erm2-nwe9', \\\n",
    "                where_string = f'({complaint_type_str}) AND latitude IS NOT NULL AND (year = 2010 OR year = 2011 OR year = 2012 OR year = 2013)')\n",
    "\n",
    "# Combine all four\n",
    "nyc_311 = nyc_311_07.append(nyc_311_08).append(nyc_311_09).append(nyc_311_10_13)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_311.complaint_type.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_311_pickle = open(f\"{data_dir}{sep}pickle{sep}nyc_311.pickle\", \"wb\")\n",
    "pickle.dump(nyc_311, nyc_311_pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull in NYC historical crime data (also uses Socrata data)\n",
    "select_string = 'cmplnt_num, cmplnt_fr_dt AS date, date_extract_y(cmplnt_fr_dt) AS year,' \\\n",
    "    'date_extract_m(cmplnt_fr_dt) AS month,  pd_cd AS class, pd_desc, law_cat_cd AS level, crm_atpt_cptd_cd AS completed, latitude, longitude'\n",
    "where_string = 'latitude IS NOT NULL AND (year = 2007 OR year = 2008 OR year = 2009 OR year = 2010 OR year = 2011 OR year = 2012 OR year = 2013)'\n",
    "nyc_crime = socrata_API_df(source_domain = \"data.cityofnewyork.us\", dataset_id = 'qgea-i56i', \\\n",
    "                           select_string = select_string, where_string = where_string, limit = 4000000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_crime_pickle = open(f\"{data_dir}{sep}pickle{sep}nyc_crime.pickle\", \"wb\")\n",
    "pickle.dump(nyc_crime, nyc_crime_pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get latitude/longitude for data\n",
    "import requests\n",
    "import json\n",
    "\n",
    "def geocode_api(lat, long):\n",
    "    '''Given a latitude and longitude, this function uses the FCC geocoding API to return the corresponding census tract FIPS code'''\n",
    "    resp = requests.get(\"https://geo.fcc.gov/api/census/block/find?latitude=\" + str(lat) + \"&longitude=\" + str(long) + \"&format=json\")\n",
    "    j = resp.json() #turn to JSON\n",
    "    fips = j['Block'][\"FIPS\"] #get census tract number from JSON\n",
    "    return fips\n",
    "\n",
    "def geocode_df(df):\n",
    "    '''Given a dataframe with two cols with latitude and longitude coordinates, this function uses the geocode_api function to \n",
    "    return a list that equals the census tract FIPS code for those lat/long coords'''\n",
    "    fips_list = []\n",
    "    lat_col = df.columns.get_loc(\"latitude\")\n",
    "    long_col = df.columns.get_loc(\"longitude\")\n",
    "    for row in range(df.shape[0]): #run through every row of the data frame\n",
    "        lat = df.iloc[row, lat_col] #get latitude coordinate for this listing\n",
    "        long = df.iloc[row, long_col] #get longitude coor for this listing\n",
    "        fips = geocode_api(lat, long) #get fips code for listing\n",
    "        fips_list.append(fips)\n",
    "    return fips_list\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
