{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and Clean Data Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Author: Lauren Thomas\n",
    "#### Created: 01/05/2021\n",
    "#### Last updated: 30/06/2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### File description: This file imports, cleans and pre-processes the data that will be used in the ML models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "import glob\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import geopandas as gp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import osmium as osm\n",
    "from os import sep\n",
    "from shapely.geometry import Point,Polygon,MultiPolygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working directory\n",
    "cwd = f\"C:{sep}Users{sep}ltswe{sep}Dropbox{sep}Oxford{sep}Thesis\"\n",
    "# Data directory is kept on flash\n",
    "data_dir = \"D:\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assign Census Tracts using Shapefiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in NYC Census Tract files\n",
    "nycct = gp.read_file(f'{cwd}{sep}data{sep}shapefiles{sep}nyct2010.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project shapefile into lat/long\n",
    "nycct_proj = nycct.to_crs(\"epsg:4326\")\n",
    "\n",
    "# Generate full geoids (state FIPS + county FIPS + CT FIPS)\n",
    "# NY State FIPS: 36\n",
    "# County fips: 061 (Manhattan), 005 (Bronx), 081 (Queens), 085 (Staten Island), 047 (Brooklyn).\n",
    "# BoroCT2010 uses a weird county code system where 1=Manhattan,2=Bronx,3=Brooklyn,4=Queens,& 5=SI\n",
    "def gen_fips(x):\n",
    "    ''' x = BoroCT2010 code'''\n",
    "    county_code = x[0:1]\n",
    "    ct_code = x[1:7]\n",
    "    if county_code == '5': #SI\n",
    "        return '36'+'085' + ct_code\n",
    "    elif county_code == '1': #Manhattan\n",
    "        return '36'+'061' + ct_code\n",
    "    elif county_code == '4': #Queens\n",
    "        return '36'+'081' + ct_code\n",
    "    elif county_code == '3': #Brooklyn\n",
    "        return '36'+'047' + ct_code\n",
    "    else: #Bronx\n",
    "        return '36'+'005' +ct_code\n",
    "\n",
    "nycct_proj['fips_code'] = nycct_proj['BoroCT2010'].apply(lambda x: gen_fips(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nycct_proj.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create spatial index for R-trees for tweets\n",
    "tweet_sindex = tweet_df.sindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of groups:\", len(tweet_sindex.leaves()), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of census tracts to iterate through\n",
    "tracts = list(nycct_proj.fips_code)\n",
    "\n",
    "\n",
    "# Iterate through tracts & find all the tweets that correspond to each given tract\n",
    "i = 1\n",
    "for tract in tracts:\n",
    "    select_tract = nycct_proj.loc[nycct_proj['fips_code']==tract]\n",
    "    \n",
    "#     # plot relevant census tract\n",
    "#     ax = select_tract.plot(color = 'red', alpha = 0.5)\n",
    "#     plt.show()\n",
    "    \n",
    "    # Get the bounding box coordinates of the census tract as a list\n",
    "    bounds = list(select_tract.bounds.values[0])\n",
    "    \n",
    "    #Get the indices of the points that are inside the bounding box of the given census tract\n",
    "    tweet_candidate_idx = list(tweet_sindex.intersection(bounds))\n",
    "    tweet_candidates = tweet_df.loc[tweet_candidate_idx]\n",
    "\n",
    "\n",
    "#     # Let's see what we have now\n",
    "#     ax = select_tract.plot(color='red', alpha = 0.5)\n",
    "#     ax = tweet_candidates.plot(ax=ax, color='blue', markersize = 2)\n",
    "#     plt.show()\n",
    "    \n",
    "\n",
    "    # Now make the precise Point in Polygon query\n",
    "    tweet_final_selection = tweet_candidates.loc[tweet_candidates.intersects(select_tract['geometry'].values[0])]\n",
    "    \n",
    "    \n",
    "#     # Let's see what we have now\n",
    "#     ax = select_tract.plot(color='red', alpha = 0.5)\n",
    "#     ax = tweet_final_selection.plot(ax=ax, color='blue', markersize = 2)\n",
    "#     plt.show()\n",
    "    \n",
    "    # Put correct tract back in original DF\n",
    "    tweet_df.loc[list(tweet_final_selection.index.values),'LocationCT'] = tract\n",
    "    \n",
    "    i+= 1\n",
    "    if i%100 == 0:\n",
    "        print(f'another hundred done! up to {i} census tracts')\n",
    "    \n",
    "# Drop any tweets that are outside NYC\n",
    "tweet_df = tweet_df[tweet_df['LocationCT'].isna() == False].reset_index(drop=True)\n",
    "display(tweet_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import & preprocess Tweet data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restructure JSONs into JSOLs (where each line = one tweet) for each month-year from Jan 2007 to Dec 2013\n",
    "# make list of month-year pairs\n",
    "ym_list = [str(year)+\"-\"+(\"0\"+str(month))[-2:] for month in range(1,13) for year in range(2007,2014)]\n",
    "\n",
    "# Create a dictionary that will contain all the JSONs for a given list of month-year pairs\n",
    "\n",
    "def create_json(ym_list, json_pickle_str):\n",
    "    all_jsons = dict()\n",
    "    for ym in ym_list:\n",
    "        print(ym)\n",
    "        # Create a list of the jsons that fall into that y-m - excluding all outputs that ends in 00000.json.\n",
    "        json_list = [j for j in glob.glob(f'{data_dir}{sep}raw_tweets{sep}{ym}*{sep}*.json', recursive=True) \n",
    "                 if j[-10:] != '00000.json']\n",
    "        # Create list of JSONs that we will append to the larger dictionary \n",
    "        temp_json_list = list()\n",
    "        for j in json_list:\n",
    "            temp_json = json.load(open(j, encoding = 'utf-8'))['data']\n",
    "            temp_json_list.extend(temp_json)\n",
    "        # Add temp_dict to larger dictionary of all JSONs with the key as the year-month\n",
    "        all_jsons[ym] = temp_json_list\n",
    "    # Pickle JSON\n",
    "    tweets_json_pickle = open(f\"{data_dir}{sep}pickle{sep}{json_pickle_str}.pickle\", \"wb\")\n",
    "    pickle.dump(all_jsons, tweets_json_pickle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create JSONs in 12 chunks (one for each month)\n",
    "# json_01 = create_json(ym_list[0:7], 'json_01')\n",
    "# json_02 = create_json(ym_list[7:14], 'json_02')\n",
    "# json_03 = create_json(ym_list[14:21], 'json_03')\n",
    "# json_04 = create_json(ym_list[21:28], 'json_04')\n",
    "# json_05 = create_json(ym_list[28:35], 'json_05')\n",
    "# json_06 = create_json(ym_list[35:42], 'json_06')\n",
    "# json_07 = create_json(ym_list[42:49], 'json_07')\n",
    "# json_08 = create_json(ym_list[49:56], 'json_08')\n",
    "# json_09 = create_json(ym_list[56:63], 'json_09')\n",
    "# json_10 = create_json(ym_list[63:70], 'json_10')\n",
    "# json_11 = create_json(ym_list[70:77], 'json_11')\n",
    "# json_12 = create_json(ym_list[77:84], 'json_12')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle_json(num):\n",
    "    ''' This function unpickles the relevant JSON'''\n",
    "    return pickle.load(open(f\"{data_dir}{sep}pickle{sep}json_{num}.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_03 = unpickle_json(\"03\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_03.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# author_id, id, lang, public_metrics.like_count, public_metrics.quote_count, public_metrics.reply_count,\n",
    "# public_metrics.retweet_count, text\n",
    "ym_list_test = ['2007-03', '2008-03','2009-03', '2010-03', '2011-03', '2012-03', '2013-03']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json = json_03\n",
    "x_list, y_list, date_list,geo_list, author_list, tweet_list, lang_list, like_list, quote_list, reply_list, retweet_list, \\\n",
    "text_list, ym_list = list(),list(),list(),list(),list(),list(),list(),list(),list(),list(),list(),list(),list()\n",
    "# Make lists of important things \n",
    "for ym in ym_list_test:\n",
    "    print(ym)\n",
    "    for j in json[ym]:\n",
    "        try:\n",
    "            geo_id = Point(j[\"geo\"][\"coordinates\"][\"coordinates\"])\n",
    "            # Annoyingly, some of the tweets are geotagged outside NYC. Get rid of these.\n",
    "            # These lat/long maxes and mins are taken from the NYC census tract shapefiles. \n",
    "            # Note x is long and y is lat (thanks, Twitter!)\n",
    "            if geo_id.x <= -73.70000924132164 and geo_id.x >= -74.25559213002796 \\\n",
    "            and geo_id.y <= 40.91553243056209 and geo_id.y >= 40.49611511946593: \n",
    "                date_list.append(j['created_at'])\n",
    "                x_list.append(geo_id.x)\n",
    "                y_list.append(geo_id.y)\n",
    "                geo_list.append(geo_id), author_list.append(j['author_id']), tweet_list.append(j['id'])\n",
    "                lang_list.append(j['lang']), like_list.append(j['public_metrics']['like_count'])\n",
    "                quote_list.append(j['public_metrics']['quote_count']), reply_list.append(j['public_metrics']['reply_count']) \n",
    "                retweet_list.append(j['public_metrics']['retweet_count']), text_list.append(j['text'])\n",
    "                ym_list.append(ym)\n",
    "        except KeyError:\n",
    "            continue\n",
    "# Create dataframe for month \n",
    "tweet_df = gp.GeoDataFrame(\n",
    "    {'ym': ym_list,\n",
    "     'date':date_list,\n",
    "    'tweet_id':tweet_list,\n",
    "     'author_id':author_list,\n",
    "    'lang': lang_list,\n",
    "    'like_count': like_list,\n",
    "     'quote_count': quote_list,\n",
    "     'reply_count': reply_list,\n",
    "     'retweet_count': retweet_list,\n",
    "     'text': text_list,\n",
    "     'x': x_list,\n",
    "     'y': y_list,\n",
    "     'geometry': geo_list\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import and preprocess other data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring in crime and 311 data, which uses the Socrata API in NYC Open Data\n",
    "# Create a function that uses the Socrata API, which is written in SoQL, a SQL-like language, to query data\n",
    "from sodapy import Socrata\n",
    "\n",
    "def socrata_API_df(source_domain, dataset_id, select_string, where_string, limit=1000):\n",
    "    '''\n",
    "    Inputs: \n",
    "    source_domain: This tells Socrata the source of the dataset you're querying\n",
    "    dataset_id: This is the unique id of the dataset\n",
    "    select_string: This string tells Socrata which variables you are selecting from the dataset\n",
    "    where_string: This string is equivalent to the \"where\" command in SQL\n",
    "    limit = This tells Socrata how many results to query. The default is 1000 b/c Socrata automatically sets it to 1000\n",
    "\n",
    "    Outputs a dataframe with with the queried results\n",
    "    '''\n",
    "    keyFile = open(f'{cwd}{sep}tokens{sep}socrata_apikey.txt', 'r')\n",
    "    token = keyFile.readline() #api token imported from txt file\n",
    "    \n",
    "    client = Socrata(source_domain, token)\n",
    "    # Change timeout var to arbitrarily large # of seconds so it doesn't time out\n",
    "    client.timeout = 50\n",
    "    results = client.get(dataset_id, limit = limit, select = select_string, where = where_string)\n",
    "    df = pd.DataFrame.from_records(results)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull in 311 and Null Data \n",
    "# 2007, 2008, & 2009 are separate; 2010-on are in a single file. \n",
    "# The only thing that changes between 2007-09 is the dataset ID, & the id + where string for 2010-on\n",
    "# so write a function that calls upon the 311 socrata API data\n",
    "# complaint type string -- separated for ease of understanding. Complaint types drawn from literature\n",
    "complaint_type_str = \"complaint_type = 'Noise - Street/Sidewalk' OR complaint_type = 'Noise - Residential' OR complaint_type = 'Noise - Vehicle' OR complaint_type = 'Street Condition' \" \\\n",
    "                    \"OR complaint_type = 'Homeless Encampment' OR complaint_type = 'Drinking' OR complaint_type = 'Noise' \" \\\n",
    "                    \"OR complaint_type = 'Noise - Park' OR complaint_type = 'Noise - House of Worship' OR complaint_type = 'HEATING' \" \\\n",
    "                    \"OR complaint_type = 'GENERAL CONSTRUCTION' OR complaint_type = 'CONSTRUCTION' OR complaint_type = 'Boilers' \" \\\n",
    "                    \"OR complaint_type = 'For Hire Vehicle Complaint' OR complaint_type = 'Bike Rack Condition' OR complaint_type = 'Illegal Parking' \" \\\n",
    "                    \"OR complaint_type = 'Building/Use' OR complaint_type = 'ELECTRIC' OR complaint_type = 'PLUMBING'\"\n",
    "\n",
    "def pull_311(dataset_id, where_string = f'latitude IS NOT NULL AND ({complaint_type_str})'):\n",
    "    return socrata_API_df(source_domain = \"data.cityofnewyork.us\", dataset_id = dataset_id, \\\n",
    "                         select_string = 'unique_key, created_date, complaint_type, date_extract_y(created_date) as year, date_extract_m(created_date) as month, descriptor, latitude, longitude', \\\n",
    "                         where_string = where_string,\n",
    "                         limit = 4000000)\n",
    "\n",
    "# 2007-2013\n",
    "nyc_311_07 = pull_311(\"aiww-p3af\")\n",
    "nyc_311_08 = pull_311('uzcy-9puk')\n",
    "nyc_311_09 = pull_311('3rfa-3xsf')\n",
    "nyc_311_10_13 = pull_311('erm2-nwe9', \\\n",
    "                where_string = f'({complaint_type_str}) AND latitude IS NOT NULL AND (year = 2010 OR year = 2011 OR year = 2012 OR year = 2013)')\n",
    "\n",
    "# Combine all four\n",
    "nyc_311 = nyc_311_07.append(nyc_311_08).append(nyc_311_09).append(nyc_311_10_13)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_311.complaint_type.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_311_pickle = open(f\"{data_dir}{sep}pickle{sep}nyc_311.pickle\", \"wb\")\n",
    "pickle.dump(nyc_311, nyc_311_pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull in NYC historical crime data (also uses Socrata data)\n",
    "select_string = 'cmplnt_num, cmplnt_fr_dt AS date, date_extract_y(cmplnt_fr_dt) AS year,' \\\n",
    "    'date_extract_m(cmplnt_fr_dt) AS month,  pd_cd AS class, pd_desc, law_cat_cd AS level, crm_atpt_cptd_cd AS completed, latitude, longitude'\n",
    "where_string = 'latitude IS NOT NULL AND (year = 2007 OR year = 2008 OR year = 2009 OR year = 2010 OR year = 2011 OR year = 2012 OR year = 2013)'\n",
    "nyc_crime = socrata_API_df(source_domain = \"data.cityofnewyork.us\", dataset_id = 'qgea-i56i', \\\n",
    "                           select_string = select_string, where_string = where_string, limit = 4000000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_crime_pickle = open(f\"{data_dir}{sep}pickle{sep}nyc_crime.pickle\", \"wb\")\n",
    "pickle.dump(nyc_crime, nyc_crime_pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring in HUD vacant addresses data\n",
    "# Create list of the excel files that will need to be loaded in \n",
    "# Glob.glob creates a list of all the files that end in .xlsx in the directory of HUD vacant data\n",
    "# The rest of the command filters out jsons that end in 00000.json since those represent meta counts and not actual tweets\n",
    "hud_list = [j for j in glob.glob(f'{data_dir}{sep}hud_vacant_data{sep}*.csv')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hud_df = pd.DataFrame()\n",
    "for file in hud_list:\n",
    "    temp_file = pd.read_csv(file, sep = None, engine='python')\n",
    "#   Using title of the file, create a column for the year & month/quarter\n",
    "    temp_file['year'] = [\"20\"+file[32:34] for i in range(temp_file.shape[0])]\n",
    "    temp_file['month'] = [file[28:30] for i in range(temp_file.shape[0])]\n",
    "    hud_df = hud_df.append(temp_file).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a FIPS code variable that is equal to the string of geoid \n",
    "# (note that b/c it's in integers, we need to re-add leading zero for states with fips codes < 10) \n",
    "hud_df['fips_code'] = hud_df[\"GEOID\"].apply(lambda x: (\"0\" + str(x))[-11:])\n",
    "\n",
    "# Create file with only NY \n",
    "ny_hud = hud_df[hud_df['fips_code'].apply(lambda x: x[0:2] == \"36\")].reset_index(drop=True)\n",
    "\n",
    "# Pickle ny hug file\n",
    "nyc_hud_pickle = open(f\"{data_dir}{sep}pickle{sep}nyc_hud.pickle\", \"wb\")\n",
    "pickle.dump(ny_hud, nyc_hud_pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpickle ny hud\n",
    "ny_hud = pickle.load(open(f\"{data_dir}{sep}pickle{sep}nyc_hud.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ny_hud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = pickle.load(open(f'{data_dir}{sep}pickle{sep}raw_tweets_df.pickle', \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
