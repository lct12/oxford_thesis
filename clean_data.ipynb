{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and Clean Data Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Author: Lauren Thomas\n",
    "#### Created: 01/05/2021\n",
    "#### Last updated: 27/06/2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### File description: This file imports, cleans and pre-processes the data that will be used in the ML models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "import glob\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import geopandas as gp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import osmium as osm\n",
    "from os import sep\n",
    "from shapely.geometry import Point,Polygon,MultiPolygon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working directory\n",
    "cwd = f\"C:{sep}Users{sep}ltswe{sep}Dropbox{sep}Oxford{sep}Thesis\"\n",
    "# Data directory is kept on flash\n",
    "data_dir = \"D:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restructure JSONs into JSOLs (where each line = one tweet) for each month-year from Jan 2007 to Dec 2013\n",
    "# make list of month-year pairs\n",
    "ym_list = [str(year)+\"-\"+(\"0\"+str(month))[-2:] for month in range(1,13) for year in range(2007,2014)]\n",
    "\n",
    "# Create a dictionary that will contain all the JSONs for a given list of month-year pairs\n",
    "\n",
    "def create_json(ym_list, json_pickle_str):\n",
    "    all_jsons = dict()\n",
    "    for ym in ym_list:\n",
    "        print(ym)\n",
    "        # Create a list of the jsons that fall into that y-m - excluding all outputs that ends in 00000.json.\n",
    "        json_list = [j for j in glob.glob(f'{data_dir}{sep}raw_tweets{sep}{ym}*{sep}*.json', recursive=True) \n",
    "                 if j[-10:] != '00000.json']\n",
    "        # Create list of JSONs that we will append to the larger dictionary \n",
    "        temp_json_list = list()\n",
    "        for j in json_list:\n",
    "            temp_json = json.load(open(j, encoding = 'utf-8'))['data']\n",
    "            temp_json_list.extend(temp_json)\n",
    "        # Add temp_dict to larger dictionary of all JSONs with the key as the year-month\n",
    "        all_jsons[ym] = temp_json_list\n",
    "    # Pickle JSON\n",
    "    tweets_json_pickle = open(f\"{data_dir}{sep}pickle{sep}{json_pickle_str}.pickle\", \"wb\")\n",
    "    pickle.dump(all_jsons, tweets_json_pickle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create JSONs in 12 chunks (one for each month)\n",
    "# json_01 = create_json(ym_list[0:7], 'json_01')\n",
    "# json_02 = create_json(ym_list[7:14], 'json_02')\n",
    "# json_03 = create_json(ym_list[14:21], 'json_03')\n",
    "# json_04 = create_json(ym_list[21:28], 'json_04')\n",
    "# json_05 = create_json(ym_list[28:35], 'json_05')\n",
    "# json_06 = create_json(ym_list[35:42], 'json_06')\n",
    "# json_07 = create_json(ym_list[42:49], 'json_07')\n",
    "# json_08 = create_json(ym_list[49:56], 'json_08')\n",
    "# json_09 = create_json(ym_list[56:63], 'json_09')\n",
    "# json_10 = create_json(ym_list[63:70], 'json_10')\n",
    "# json_11 = create_json(ym_list[70:77], 'json_11')\n",
    "# json_12 = create_json(ym_list[77:84], 'json_12')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle_json(num):\n",
    "    ''' This function unpickles the relevant JSON'''\n",
    "    return pickle.load(open(f\"{data_dir}{sep}pickle{sep}json_{num}.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_03 = unpickle_json(\"03\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# author_id, id, lang, public_metrics.like_count, public_metrics.quote_count, public_metrics.reply_count,\n",
    "# public_metrics.retweet_count, text\n",
    "ym_list_test = ['2007-03', '2008-03','2009-03', '2010-03', '2011-03', '2012-03', '2013-03']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_list, y_list, geo_list, author_list, tweet_list, lang_list, like_list, quote_list, reply_list, retweet_list, \\\n",
    "text_list, ym_list = list(),list(),list(),list(),list(),list(),list(),list(),list(),list(),list(),list()\n",
    "# Make lists of important things \n",
    "for ym in ym_list_test:\n",
    "    print(ym)\n",
    "    for j in json_03[ym]:\n",
    "        try:\n",
    "            geo_id = Point(j[\"geo\"][\"coordinates\"][\"coordinates\"])\n",
    "            x_list.append(geo_id.x)\n",
    "            y_list.append(geo_id.y)\n",
    "            geo_list.append(geo_id), author_list.append(j['author_id']), tweet_list.append(j['id'])\n",
    "            lang_list.append(j['lang']), like_list.append(j['public_metrics']['like_count'])\n",
    "            quote_list.append(j['public_metrics']['quote_count']), reply_list.append(j['public_metrics']['reply_count']) \n",
    "            retweet_list.append(j['public_metrics']['retweet_count']), text_list.append(j['text'])\n",
    "            ym_list.append(ym)\n",
    "        except KeyError:\n",
    "            continue\n",
    "# Create dataframe for month \n",
    "tst_df = pd.DataFrame(\n",
    "    {'ym': ym_list,\n",
    "    'tweet_id':tweet_list,\n",
    "     'author_id':author_list,\n",
    "    'lang': lang_list,\n",
    "    'like_count': like_list,\n",
    "     'quote_count': quote_list,\n",
    "     'reply_count': reply_list,\n",
    "     'retweet_count': retweet_list,\n",
    "     'text': text_list,\n",
    "     'x': x_list,\n",
    "     'y': y_list,\n",
    "     'geometry': geo_list\n",
    "    }\n",
    ")\n",
    "tst_geodf = gp.GeoDataFrame(tst_df[['x', 'y', 'geometry']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_geodf = tst_df['geometry']\n",
    "tst_geodf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from zipfile import ZipFile\n",
    "from io import BytesIO\n",
    "\n",
    "# Download and unzip shape files from census tract website\n",
    "# Begin with a function that downloads & unzips a url\n",
    "def download_url(url, save_path):\n",
    "    with urlopen(url) as zipresp:\n",
    "        with ZipFile(BytesIO(zipresp.read())) as zfile:\n",
    "            zfile.extractall(save_path)\n",
    "    \n",
    "\n",
    "# Create a function that, when given a list of urls, downloads and unzips each url in the list & makes into pandas\n",
    "def download_unzip(url_start, url_list, save_path):\n",
    "    '''\n",
    "    url_start = first part of the url (that is the same for all the zip files)\n",
    "    url_list = second part (the part that changes & is the name of the unzipped file) \n",
    "    '''\n",
    "    # Create empty df\n",
    "    shape_df = pd.DataFrame()\n",
    "    # Put each shape file into a df & append\n",
    "    for url in url_list:\n",
    "        download_url(url_start+url+\".zip\", save_path)\n",
    "        shape_df = shape_df.append(gp.read_file(f\"{save_path}{sep}{url}.shp\"))\n",
    "    return shape_df\n",
    "\n",
    "# Create list of unique urls (tl_2013_##_tract) where ## = 01-78, with some numbers not included\n",
    "# to_drop = list of #'s not to include\n",
    "to_drop = [3, 7, 14,43,52,57,58,59,61,62,63,64,65,67,68,70,71,73,74,75,76,77]\n",
    "url_list = [\"tl_2013_\" + (\"0\"+str(i))[-2:] + \"_tract\" for i in range(1,79) if i not in to_drop]\n",
    "url_start = \"https://www2.census.gov/geo/tiger/TIGER2013/TRACT/\"\n",
    "\n",
    "# Create shapefile dataframe of all counties in NYC\n",
    "shape_df = download_unzip(url_start, url_list, \n",
    "                          f\"{data_dir}{sep}shape_files\")\n",
    "        \n",
    "# Get rid of any non-NYC counties (those not in the five NYC counties)\n",
    "nyc_counties = shape_df[(shape_df['COUNTYFP'] == '069') | (shape_df['COUNTYFP'] == '047') | (shape_df['COUNTYFP'] == '081') \n",
    "    | (shape_df['COUNTYFP'] == '085') | (shape_df['COUNTYFP'] == '005')].reset_index(drop=True)\n",
    "        \n",
    "# Pickle dataframe\n",
    "nyc_counties_pickle = open(f\"{data_dir}{sep}pickle{sep}nyc_counties.pickle\", \"wb\")\n",
    "pickle.dump(nyc_counties, nyc_counties_pickle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_counties = pickle.load(open(f\"{data_dir}{sep}pickle{sep}nyc_counties.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_counties.geometry[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = Point(all_jsons['2011-02'][0]['geo']['coordinates']['coordinates'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that downloads, unzips all the shape files from census tract website,\n",
    "# then creates a dataframe from the shape file, which it appends to the others\n",
    "# then create a function that takes a point from the tweet, runs it through all the rows, finds relevant\n",
    "# census tract, then returns the relevant census tract\n",
    "# then assign census tract to tweet & creates field in JSON\n",
    "# do this for all tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_column = nyc_counties.columns.get_loc(\"geometry\")\n",
    "geoid_column = nyc_counties.columns.get_loc(\"GEOID\")\n",
    "for row in range(nyc_counties.shape[0]):\n",
    "    poly = nyc_counties.iloc[row,poly_column]\n",
    "    if p1.within(poly) == True:\n",
    "        print(nyc_counties.iloc[row,geoid_column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict = {temp_json2[i]['id']:temp_json2[i] for i in range(len(temp_json2))}\n",
    "# test_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring in crime and 311 data, which uses the Socrata API in NYC Open Data\n",
    "# Create a function that uses the Socrata API, which is written in SoQL, a SQL-like language, to query data\n",
    "from sodapy import Socrata\n",
    "\n",
    "def socrata_API_df(source_domain, dataset_id, select_string, where_string, limit=1000):\n",
    "    '''\n",
    "    Inputs: \n",
    "    source_domain: This tells Socrata the source of the dataset you're querying\n",
    "    dataset_id: This is the unique id of the dataset\n",
    "    select_string: This string tells Socrata which variables you are selecting from the dataset\n",
    "    where_string: This string is equivalent to the \"where\" command in SQL\n",
    "    limit = This tells Socrata how many results to query. The default is 1000 b/c Socrata automatically sets it to 1000\n",
    "\n",
    "    Outputs a dataframe with with the queried results\n",
    "    '''\n",
    "    keyFile = open(f'{cwd}{sep}tokens{sep}socrata_apikey.txt', 'r')\n",
    "    token = keyFile.readline() #api token imported from txt file\n",
    "    \n",
    "    client = Socrata(source_domain, token)\n",
    "    # Change timeout var to arbitrarily large # of seconds so it doesn't time out\n",
    "    client.timeout = 50\n",
    "    results = client.get(dataset_id, limit = limit, select = select_string, where = where_string)\n",
    "    df = pd.DataFrame.from_records(results)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to shorten lat/long\n",
    "# Cut lat/long to 4 decimals (10 m)\n",
    "def round_lat_long(df):\n",
    "    df[\"latitude\"] = df[\"latitude\"].apply(lambda x: round(float(x),3))\n",
    "    df[\"longitude\"]= df[\"longitude\"].apply(lambda x: round(float(x),3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull in 311 and Null Data \n",
    "# 2007, 2008, & 2009 are separate; 2010-on are in a single file. \n",
    "# The only thing that changes between 2007-09 is the dataset ID, & the id + where string for 2010-on\n",
    "# so write a function that calls upon the 311 socrata API data\n",
    "# complaint type string -- separated for ease of understanding. Complaint types drawn from literature\n",
    "complaint_type_str = \"complaint_type = 'Noise - Street/Sidewalk' OR complaint_type = 'Noise - Residential' OR complaint_type = 'Noise - Vehicle' OR complaint_type = 'Street Condition' \" \\\n",
    "                    \"OR complaint_type = 'Homeless Encampment' OR complaint_type = 'Drinking' OR complaint_type = 'Noise' \" \\\n",
    "                    \"OR complaint_type = 'Noise - Park' OR complaint_type = 'Noise - House of Worship' OR complaint_type = 'HEATING' \" \\\n",
    "                    \"OR complaint_type = 'GENERAL CONSTRUCTION' OR complaint_type = 'CONSTRUCTION' OR complaint_type = 'Boilers' \" \\\n",
    "                    \"OR complaint_type = 'For Hire Vehicle Complaint' OR complaint_type = 'Bike Rack Condition' OR complaint_type = 'Illegal Parking' \" \\\n",
    "                    \"OR complaint_type = 'Building/Use' OR complaint_type = 'ELECTRIC' OR complaint_type = 'PLUMBING'\"\n",
    "\n",
    "def pull_311(dataset_id, where_string = f'latitude IS NOT NULL AND ({complaint_type_str})'):\n",
    "    return socrata_API_df(source_domain = \"data.cityofnewyork.us\", dataset_id = dataset_id, \\\n",
    "                         select_string = 'unique_key, created_date, complaint_type, date_extract_y(created_date) as year, date_extract_m(created_date) as month, descriptor, latitude, longitude', \\\n",
    "                         where_string = where_string,\n",
    "                         limit = 4000000)\n",
    "\n",
    "# 2007-2013\n",
    "nyc_311_07 = pull_311(\"aiww-p3af\")\n",
    "nyc_311_08 = pull_311('uzcy-9puk')\n",
    "nyc_311_09 = pull_311('3rfa-3xsf')\n",
    "nyc_311_10_13 = pull_311('erm2-nwe9', \\\n",
    "                where_string = f'({complaint_type_str}) AND latitude IS NOT NULL AND (year = 2010 OR year = 2011 OR year = 2012 OR year = 2013)')\n",
    "\n",
    "# Combine all four\n",
    "nyc_311 = nyc_311_07.append(nyc_311_08).append(nyc_311_09).append(nyc_311_10_13)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_311.complaint_type.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_311_pickle = open(f\"{data_dir}{sep}pickle{sep}nyc_311.pickle\", \"wb\")\n",
    "pickle.dump(nyc_311, nyc_311_pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull in NYC historical crime data (also uses Socrata data)\n",
    "select_string = 'cmplnt_num, cmplnt_fr_dt AS date, date_extract_y(cmplnt_fr_dt) AS year,' \\\n",
    "    'date_extract_m(cmplnt_fr_dt) AS month,  pd_cd AS class, pd_desc, law_cat_cd AS level, crm_atpt_cptd_cd AS completed, latitude, longitude'\n",
    "where_string = 'latitude IS NOT NULL AND (year = 2007 OR year = 2008 OR year = 2009 OR year = 2010 OR year = 2011 OR year = 2012 OR year = 2013)'\n",
    "nyc_crime = socrata_API_df(source_domain = \"data.cityofnewyork.us\", dataset_id = 'qgea-i56i', \\\n",
    "                           select_string = select_string, where_string = where_string, limit = 4000000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_crime_pickle = open(f\"{data_dir}{sep}pickle{sep}nyc_crime.pickle\", \"wb\")\n",
    "pickle.dump(nyc_crime, nyc_crime_pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring in HUD vacant addresses data\n",
    "# Create list of the excel files that will need to be loaded in \n",
    "# Glob.glob creates a list of all the files that end in .xlsx in the directory of HUD vacant data\n",
    "# The rest of the command filters out jsons that end in 00000.json since those represent meta counts and not actual tweets\n",
    "hud_list = [j for j in glob.glob(f'{data_dir}{sep}hud_vacant_data{sep}*.csv')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hud_df = pd.DataFrame()\n",
    "for file in hud_list:\n",
    "    temp_file = pd.read_csv(file, sep = None, engine='python')\n",
    "#   Using title of the file, create a column for the year & month/quarter\n",
    "    temp_file['year'] = [\"20\"+file[32:34] for i in range(temp_file.shape[0])]\n",
    "    temp_file['month'] = [file[28:30] for i in range(temp_file.shape[0])]\n",
    "    hud_df = hud_df.append(temp_file).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a FIPS code variable that is equal to the string of geoid \n",
    "# (note that b/c it's in integers, we need to re-add leading zero for states with fips codes < 10) \n",
    "hud_df['fips_code'] = hud_df[\"GEOID\"].apply(lambda x: (\"0\" + str(x))[-11:])\n",
    "\n",
    "# Create file with only NY \n",
    "ny_hud = hud_df[hud_df['fips_code'].apply(lambda x: x[0:2] == \"36\")].reset_index(drop=True)\n",
    "\n",
    "# Pickle ny hug file\n",
    "nyc_hud_pickle = open(f\"{data_dir}{sep}pickle{sep}nyc_hud.pickle\", \"wb\")\n",
    "pickle.dump(ny_hud, nyc_hud_pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpickle ny hud\n",
    "ny_hud = pickle.load(open(f\"{data_dir}{sep}pickle{sep}nyc_hud.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ny_hud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = pickle.load(open(f'{data_dir}{sep}pickle{sep}raw_tweets_df.pickle', \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
