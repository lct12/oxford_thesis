{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and Clean Data Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Author: Lauren Thomas\n",
    "#### Created: 01/05/2021\n",
    "#### Last updated: 20/07/2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### File description: This file imports, cleans and pre-processes the data that will be used in the ML models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ltswe\\anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\ltswe\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.GK7GX5KEQ4F6UYO3P26ULGBQYHGQO7J4.gfortran-win_amd64.dll\n",
      "C:\\Users\\ltswe\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.QVLO2T66WEPI7JZ63PS3HMOHFEY472BC.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gzip\n",
    "import glob\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import geopandas as gp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import osmium as osm\n",
    "from os import sep\n",
    "from shapely.geometry import Point,Polygon,MultiPolygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working directory\n",
    "cwd = f\"C:{sep}Users{sep}ltswe{sep}Dropbox{sep}Oxford{sep}Thesis\"\n",
    "# Data directory is kept on flash\n",
    "data_dir = \"D:\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assign Census Tracts using Shapefiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in NYC Census Tract files\n",
    "nycct = gp.read_file(f'{cwd}{sep}data{sep}shapefiles{sep}nyct2010.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project shapefile into lat/long\n",
    "nycct_proj = nycct.to_crs(\"epsg:4326\")\n",
    "\n",
    "# Generate full geoids (state FIPS + county FIPS + CT FIPS)\n",
    "# NY State FIPS: 36\n",
    "# County fips: 061 (Manhattan), 005 (Bronx), 081 (Queens), 085 (Staten Island), 047 (Brooklyn).\n",
    "# BoroCT2010 uses a weird county code system where 1=Manhattan,2=Bronx,3=Brooklyn,4=Queens,& 5=SI\n",
    "def gen_fips(x):\n",
    "    ''' x = BoroCT2010 code'''\n",
    "    county_code = x[0:1]\n",
    "    ct_code = x[1:7]\n",
    "    if county_code == '5': #SI\n",
    "        return '36'+'085' + ct_code\n",
    "    elif county_code == '1': #Manhattan\n",
    "        return '36'+'061' + ct_code\n",
    "    elif county_code == '4': #Queens\n",
    "        return '36'+'081' + ct_code\n",
    "    elif county_code == '3': #Brooklyn\n",
    "        return '36'+'047' + ct_code\n",
    "    else: #Bronx\n",
    "        return '36'+'005' +ct_code\n",
    "\n",
    "nycct_proj['fips_code'] = nycct_proj['BoroCT2010'].apply(lambda x: gen_fips(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nycct_proj.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that rounds lat/long in df to six digits (what Twitter provides for lat/long)\n",
    "def round_lat_long(df):\n",
    "    df['latitude'] = df['latitude'].apply(lambda x: round(float(x), 6))\n",
    "    df['longitude'] = df['longitude'].apply(lambda x: round(float(x), 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function that assigns census tract using R-tree spatial indices \n",
    "def assign_ct(geo_df, df_name_str, count_thousand=True,count_hundred=False):\n",
    "    ''' \n",
    "    This function assigns a census tract to each observation in a GeoDataFrame using the nycct file created above. \n",
    "    Input: geo_df: This is a GeoDataFrame that contains a col entitled 'geometry' that has lat/long points\n",
    "    df_name: This is a string that contains the name of the dataframe \n",
    "    count_thousand: If true, tells you when every 1000 census tracts have been assigned\n",
    "    count_hundred: If true, tells you when every 100 census tracts have been assigned\n",
    "    Output: The original GeoDataFrame with a new col entitled 'LocationCT' that has the census tract corresponding\n",
    "    to each observation. \n",
    "    '''\n",
    "    print(\"Assigning Census Tracts for DataFrame\", df_name_str)\n",
    "    \n",
    "    # Begin by creating a spatial index for the R-trees for the Geodataframe\n",
    "    df_sindex = geo_df.sindex\n",
    "    print(f'Number of groups for dataframe {df_name_str}: ', len(df_sindex.leaves()), '\\n')\n",
    "    \n",
    "    # Create a list of census tracts to iterate through\n",
    "    tracts = list(nycct_proj.fips_code)\n",
    "\n",
    "\n",
    "    # Iterate through tracts & find all the tweets that correspond to each given tract\n",
    "    i = 1\n",
    "    for tract in tracts:\n",
    "        select_tract = nycct_proj.loc[nycct_proj['fips_code']==tract]\n",
    "    \n",
    "        # Get the bounding box coordinates of the census tract as a list\n",
    "        bounds = list(select_tract.bounds.values[0])\n",
    "    \n",
    "        #Get the indices of the points that are inside the bounding box of the given census tract\n",
    "        df_candidate_idx = list(df_sindex.intersection(bounds))\n",
    "        df_candidates = geo_df.loc[df_candidate_idx]\n",
    "\n",
    "        # Now make the precise Point in Polygon query\n",
    "        df_final_selection = df_candidates.loc[df_candidates.intersects(select_tract['geometry'].values[0])]\n",
    "    \n",
    "        # Put correct tract back in original DF\n",
    "        geo_df.loc[list(df_final_selection.index.values),'LocationCT'] = tract\n",
    "    \n",
    "        i+= 1\n",
    "        if count_thousand == True:\n",
    "            if i%1000 == 0:\n",
    "                print(f'another thousand done! up to {i} census tracts for DataFrame {df_name_str}')\n",
    "        elif count_hundred == True:\n",
    "            if i%100 == 0:\n",
    "                print(f'another hundred done! up to {i} census tracts for DataFrame {df_name_str}')\n",
    "    \n",
    "    # Drop any tweets that are outside NYC\n",
    "    geo_df = geo_df[geo_df['LocationCT'].isna() == False].reset_index(drop=True)\n",
    "    \n",
    "    # Pickle dataframe\n",
    "    df_pickle = open(f\"{data_dir}{sep}pickle{sep}{df_name_str}.pickle\", \"wb\")\n",
    "    pickle.dump(geo_df, df_pickle)\n",
    "    \n",
    "    print(f'DataFrame {df_name_str} is all pickled and ready to go! \\n')\n",
    "    return geo_df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import & preprocess Tweet data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restructure JSONs into JSOLs (where each line = one tweet) for each month-year from Jan 2011 to Dec 2013\n",
    "# make list of month-year pairs\n",
    "ym_list = [str(year)+\"-\"+(\"0\"+str(month))[-2:] for month in range(1,13) for year in range(2011,2014)]\n",
    "\n",
    "# Create a dictionary that will contain all the JSONs for a given list of month-year pairs\n",
    "\n",
    "def create_json(ym_list, json_pickle_str):\n",
    "    all_jsons = dict()\n",
    "    for ym in ym_list:\n",
    "        print(ym)\n",
    "        # Create a list of the jsons that fall into that y-m - excluding all outputs that ends in 00000.json.\n",
    "        json_list = [j for j in glob.glob(f'{data_dir}{sep}raw_tweets{sep}{ym}*{sep}*.json', recursive=True) \n",
    "                 if j[-10:] != '00000.json']\n",
    "        # Create list of JSONs that we will append to the larger dictionary \n",
    "        temp_json_list = list()\n",
    "        for j in json_list:\n",
    "            temp_json = json.load(open(j, encoding = 'utf-8'))['data']\n",
    "            temp_json_list.extend(temp_json)\n",
    "        # Add temp_dict to larger dictionary of all JSONs with the key as the year-month\n",
    "        all_jsons[ym] = temp_json_list\n",
    "    # Pickle JSON\n",
    "    tweets_json_pickle = open(f\"{data_dir}{sep}pickle{sep}{json_pickle_str}.pickle\", \"wb\")\n",
    "    pickle.dump(all_jsons, tweets_json_pickle)\n",
    "    print(json_pickle_str, \"pickled!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create JSONs in 12 chunks (one for each month; 3 years in total)\n",
    "# json_01 = create_json(ym_list[0:3], 'json_01')\n",
    "# json_02 = create_json(ym_list[3:6], 'json_02')\n",
    "# json_03 = create_json(ym_list[6:9], 'json_03')\n",
    "# json_04 = create_json(ym_list[9:12], 'json_04')\n",
    "# json_05 = create_json(ym_list[12:15], 'json_05')\n",
    "# json_06 = create_json(ym_list[15:18], 'json_06')\n",
    "# json_07 = create_json(ym_list[18:21], 'json_07')\n",
    "# json_08 = create_json(ym_list[21:24], 'json_08')\n",
    "# json_09 = create_json(ym_list[24:27], 'json_09')\n",
    "# json_10 = create_json(ym_list[27:30], 'json_10')\n",
    "# json_11 = create_json(ym_list[30:33], 'json_11')\n",
    "# json_12 = create_json(ym_list[33:36], 'json_12')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle_json(num):\n",
    "    ''' This function unpickles the relevant JSON'''\n",
    "    return pickle.load(open(f\"{data_dir}{sep}pickle{sep}json_{num}.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tweet_gdf(json_num, json_ym_list): \n",
    "    '''\n",
    "    This function creates a GeoDataFrame of the tweets in a given JSON with the important info from the JSON\n",
    "    Input: json_num: month number of relevant JSON (in string form, like \"01\" for Jan, \"02\" for Feb, etc.)\n",
    "    json_ym_list: extract of ym_list that contains the relevant month-year pairs (e.g. ym_list[0:7] for Jan, etc.)\n",
    "    Output: GeoDataFrame of tweets in JSON that contain geo-coordinates\n",
    "    '''\n",
    "    print(\"Creating GeoDataFrame\", json_num)\n",
    "    json = unpickle_json(json_num)\n",
    "\n",
    "    x_list, y_list, date_list,geo_list, author_list, tweet_list, lang_list, like_list, quote_list, reply_list, retweet_list, \\\n",
    "    text_list, ym_list = list(),list(),list(),list(),list(),list(),list(),list(),list(),list(),list(),list(),list()\n",
    "    # Make lists to form columns in GeoDataFrame \n",
    "    for ym in json_ym_list:\n",
    "        print(ym)\n",
    "        for j in json[ym]:\n",
    "            # try/except: if KeyError occurs, it means it doesn't have geo-coordinates, so we ignore those\n",
    "            try:\n",
    "                geo_id = Point(j[\"geo\"][\"coordinates\"][\"coordinates\"])\n",
    "                # Annoyingly, some of the tweets are geotagged outside NYC. Get rid of these.\n",
    "                # These lat/long maxes and mins are taken from the NYC census tract shapefiles. \n",
    "                # Note x is long and y is lat (thanks, Twitter!)\n",
    "                if geo_id.x <= -73.70000924132164 and geo_id.x >= -74.25559213002796 \\\n",
    "                and geo_id.y <= 40.91553243056209 and geo_id.y >= 40.49611511946593: \n",
    "                    date_list.append(j['created_at'])\n",
    "                    x_list.append(geo_id.x)\n",
    "                    y_list.append(geo_id.y)\n",
    "                    geo_list.append(geo_id), author_list.append(j['author_id']), tweet_list.append(j['id'])\n",
    "                    lang_list.append(j['lang']), like_list.append(j['public_metrics']['like_count'])\n",
    "                    quote_list.append(j['public_metrics']['quote_count']), reply_list.append(j['public_metrics']['reply_count']) \n",
    "                    retweet_list.append(j['public_metrics']['retweet_count']), text_list.append(j['text'])\n",
    "                    ym_list.append(ym)\n",
    "            except KeyError:\n",
    "                continue\n",
    "    # Create geodataframe for given json (month) \n",
    "    tweet_df = gp.GeoDataFrame(\n",
    "        {'ym': ym_list,\n",
    "         'date':date_list,\n",
    "        'tweet_id':tweet_list,\n",
    "         'author_id':author_list,\n",
    "        'lang': lang_list,\n",
    "        'like_count': like_list,\n",
    "         'quote_count': quote_list,\n",
    "         'reply_count': reply_list,\n",
    "         'retweet_count': retweet_list,\n",
    "         'text': text_list,\n",
    "         'x': x_list,\n",
    "         'y': y_list,\n",
    "         'geometry': geo_list\n",
    "        }\n",
    "    )\n",
    "    # check for duplicates and drop any\n",
    "    tweet_df = tweet_df.drop_duplicates()\n",
    "    print(\"\\n\")\n",
    "    return tweet_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the create_gdf function to create GeoDataFrames for all the JSONs\n",
    "tweet_gdf_01 = create_tweet_gdf('01',ym_list[0:3])\n",
    "tweet_gdf_02 = create_tweet_gdf('02',ym_list[3:6])\n",
    "tweet_gdf_03 = create_tweet_gdf(\"03\", ym_list[6:9])\n",
    "tweet_gdf_04 = create_tweet_gdf('04',ym_list[9:12])\n",
    "tweet_gdf_05 = create_tweet_gdf('05',ym_list[12:15])\n",
    "tweet_gdf_06 = create_tweet_gdf('06',ym_list[15:18])\n",
    "tweet_gdf_07 = create_tweet_gdf('07',ym_list[18:21])\n",
    "tweet_gdf_08 = create_tweet_gdf('08',ym_list[21:24])\n",
    "tweet_gdf_09 = create_tweet_gdf('09',ym_list[24:27])\n",
    "tweet_gdf_10 = create_tweet_gdf('10',ym_list[27:30])\n",
    "tweet_gdf_11 = create_tweet_gdf('11',ym_list[30:33])\n",
    "tweet_gdf_12 = create_tweet_gdf('12',ym_list[33:36])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the assign_ct function to assign census tracts to obs in each of the geodataframes\n",
    "tweet_gdf_01 = assign_ct(tweet_gdf_01, 'tweet_gdf_01')\n",
    "tweet_gdf_02 = assign_ct(tweet_gdf_02, 'tweet_gdf_02')\n",
    "tweet_gdf_03 = assign_ct(tweet_gdf_03, 'tweet_gdf_03')\n",
    "tweet_gdf_04 = assign_ct(tweet_gdf_04, 'tweet_gdf_04')\n",
    "tweet_gdf_05 = assign_ct(tweet_gdf_05, 'tweet_gdf_05')\n",
    "tweet_gdf_06 = assign_ct(tweet_gdf_06, 'tweet_gdf_06')\n",
    "tweet_gdf_07 = assign_ct(tweet_gdf_07, 'tweet_gdf_07')\n",
    "tweet_gdf_08 = assign_ct(tweet_gdf_08, 'tweet_gdf_08')\n",
    "tweet_gdf_09 = assign_ct(tweet_gdf_09, 'tweet_gdf_09')\n",
    "tweet_gdf_10 = assign_ct(tweet_gdf_10, 'tweet_gdf_10')\n",
    "tweet_gdf_11 = assign_ct(tweet_gdf_11, 'tweet_gdf_11')\n",
    "tweet_gdf_12 = assign_ct(tweet_gdf_12, 'tweet_gdf_12')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_gdf_01 = pickle.load(open(f'{data_dir}{sep}pickle{sep}tweet_gdf_01.pickle', 'rb'))\n",
    "tweet_gdf_02 = pickle.load(open(f'{data_dir}{sep}pickle{sep}tweet_gdf_02.pickle', 'rb'))\n",
    "tweet_gdf_03 = pickle.load(open(f'{data_dir}{sep}pickle{sep}tweet_gdf_03.pickle', 'rb'))\n",
    "tweet_gdf_04 = pickle.load(open(f'{data_dir}{sep}pickle{sep}tweet_gdf_04.pickle', 'rb'))\n",
    "tweet_gdf_05 = pickle.load(open(f'{data_dir}{sep}pickle{sep}tweet_gdf_05.pickle', 'rb'))\n",
    "tweet_gdf_06 = pickle.load(open(f'{data_dir}{sep}pickle{sep}tweet_gdf_06.pickle', 'rb'))\n",
    "tweet_gdf_07= pickle.load(open(f'{data_dir}{sep}pickle{sep}tweet_gdf_07.pickle', 'rb'))\n",
    "tweet_gdf_08 = pickle.load(open(f'{data_dir}{sep}pickle{sep}tweet_gdf_08.pickle', 'rb'))\n",
    "tweet_gdf_09 = pickle.load(open(f'{data_dir}{sep}pickle{sep}tweet_gdf_09.pickle', 'rb'))\n",
    "tweet_gdf_10 = pickle.load(open(f'{data_dir}{sep}pickle{sep}tweet_gdf_10.pickle', 'rb'))\n",
    "tweet_gdf_11 = pickle.load(open(f'{data_dir}{sep}pickle{sep}tweet_gdf_11.pickle', 'rb'))\n",
    "tweet_gdf_12 = pickle.load(open(f'{data_dir}{sep}pickle{sep}tweet_gdf_12.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "tweet_gdf_list = [tweet_gdf_01, tweet_gdf_02, tweet_gdf_03, tweet_gdf_04, tweet_gdf_05, tweet_gdf_06, tweet_gdf_07,\n",
    "          tweet_gdf_08, tweet_gdf_09, tweet_gdf_10, tweet_gdf_11, tweet_gdf_12]\n",
    "for df in tweet_gdf_list:\n",
    "    i += len(df)\n",
    "print(i,'tweets in total')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create heat map for tweet counts per census tract over time\n",
    "def collapse_append_df(df, df_to_append, collapse_function = \"count\"):\n",
    "    ''' This function will collapse df by census tract/year and append it to df_to_append'''\n",
    "    # Get the year and add it as a column\n",
    "    df['year'] = pd.DatetimeIndex(df['date']).year\n",
    "    if collapse_function == 'count':\n",
    "        collapsed_df = df[['ym', 'LocationCT', 'year']].groupby([\"LocationCT\", \"year\"]) \\\n",
    "        .agg('count').rename(columns={'ym': 'count'}).reset_index(level=['LocationCT', 'year'])\n",
    "    return df_to_append.append(collapsed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collapsed_count = pd.DataFrame()\n",
    "# collapse each of the gdfs\n",
    "for df in tweet_gdf_list:\n",
    "    collapsed_count = collapse_append_df(df, collapsed_count)\n",
    "# Add together the months of each year\n",
    "collapsed_count = collapsed_count.groupby(['LocationCT', 'year']) \\\n",
    ".agg('sum').reset_index(level=['LocationCT', 'year'])\n",
    "\n",
    "# Merge geometries back in using projected shapefile. Use a right merge to ensure all census tracts are \n",
    "# included (b/c not all census tracts are in the collapsed count for every year)\n",
    "to_merge = nycct_proj[['geometry', 'fips_code']].rename(columns={'fips_code': 'LocationCT'})\n",
    "collapsed_count_gdf = gp.GeoDataFrame(collapsed_count.merge(to_merge, how='right', on='LocationCT'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create heat map over time using collapsed_count_gdf\n",
    "def create_heat_map(df, year, title, save_as):\n",
    "    df[df['year'] == year].plot(column='count', legend=True)\n",
    "    plt.title(f\"{title}\")\n",
    "    plt.savefig(f\"{cwd}{sep}figures{sep}{save_as}.jpg\")\n",
    "    plt.show()\n",
    "    \n",
    "for year in [2011,2012,2013]:\n",
    "    year_str = str(year)\n",
    "    create_heat_map(collapsed_count_gdf, year, f'Count of Geotagged Tweets by Census Tract in {year_str}',\n",
    "                   f'geotagged_tweets_ct_{year_str}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collapsed_count_gdf['count'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import and preprocess other data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 311 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring in crime and 311 data, which uses the Socrata API in NYC Open Data\n",
    "# Create a function that uses the Socrata API, which is written in SoQL, a SQL-like language, to query data\n",
    "from sodapy import Socrata\n",
    "\n",
    "def socrata_API_df(source_domain, dataset_id, select_string, where_string, limit=1000):\n",
    "    '''\n",
    "    Inputs: \n",
    "    source_domain: This tells Socrata the source of the dataset you're querying\n",
    "    dataset_id: This is the unique id of the dataset\n",
    "    select_string: This string tells Socrata which variables you are selecting from the dataset\n",
    "    where_string: This string is equivalent to the \"where\" command in SQL\n",
    "    limit = This tells Socrata how many results to query. The default is 1000 b/c Socrata automatically sets it to 1000\n",
    "\n",
    "    Outputs a dataframe with with the queried results\n",
    "    '''\n",
    "    keyFile = open(f'{cwd}{sep}tokens{sep}socrata_apikey.txt', 'r')\n",
    "    token = keyFile.readline() #api token imported from txt file\n",
    "    \n",
    "    client = Socrata(source_domain, token)\n",
    "    # Change timeout var to arbitrarily large # of seconds so it doesn't time out\n",
    "    client.timeout = 50\n",
    "    results = client.get(dataset_id, limit = limit, select = select_string, where = where_string)\n",
    "    df = pd.DataFrame.from_records(results)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull in 311 and Null Data \n",
    "# 2007, 2008, & 2009 are separate; 2010-on are in a single file. \n",
    "# The only thing that changes between 2007-09 is the dataset ID, & the id + where string for 2010-on\n",
    "# so write a function that calls upon the 311 socrata API data\n",
    "# complaint type string -- separated for ease of understanding. Complaint types drawn from literature\n",
    "complaint_type_str = \"complaint_type = 'Noise - Street/Sidewalk' OR complaint_type = 'Noise - Residential' OR complaint_type = 'Noise - Vehicle' OR complaint_type = 'Street Condition' \" \\\n",
    "                    \"OR complaint_type = 'Homeless Encampment' OR complaint_type = 'Drinking' OR complaint_type = 'Noise' \" \\\n",
    "                    \"OR complaint_type = 'Noise - Park' OR complaint_type = 'Noise - House of Worship' OR complaint_type = 'HEATING' \" \\\n",
    "                    \"OR complaint_type = 'GENERAL CONSTRUCTION' OR complaint_type = 'CONSTRUCTION' OR complaint_type = 'Boilers' \" \\\n",
    "                    \"OR complaint_type = 'For Hire Vehicle Complaint' OR complaint_type = 'Bike Rack Condition' OR complaint_type = 'Illegal Parking' \" \\\n",
    "                    \"OR complaint_type = 'Building/Use' OR complaint_type = 'ELECTRIC' OR complaint_type = 'PLUMBING'\"\n",
    "\n",
    "def pull_311(dataset_id, where_string = f'latitude IS NOT NULL AND ({complaint_type_str})'):\n",
    "    return socrata_API_df(source_domain = \"data.cityofnewyork.us\", dataset_id = dataset_id, \\\n",
    "                         select_string = 'unique_key, created_date, complaint_type, date_extract_y(created_date) as year, date_extract_m(created_date) as month, descriptor, latitude, longitude', \\\n",
    "                         where_string = where_string,\n",
    "                         limit = 4000000)\n",
    "\n",
    "# 2007-2013\n",
    "nyc_311_07 = pull_311(\"aiww-p3af\")\n",
    "nyc_311_08 = pull_311('uzcy-9puk')\n",
    "nyc_311_09 = pull_311('3rfa-3xsf')\n",
    "nyc_311_10_13 = pull_311('erm2-nwe9', \\\n",
    "                where_string = f'({complaint_type_str}) AND latitude IS NOT NULL AND (year = 2010 OR year = 2011 OR year = 2012 OR year = 2013)')\n",
    "\n",
    "# Combine all four\n",
    "nyc_311 = nyc_311_07.append(nyc_311_08).append(nyc_311_09).append(nyc_311_10_13).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_311.complaint_type.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_311_pickle = open(f\"{data_dir}{sep}pickle{sep}nyc_311.pickle\", \"wb\")\n",
    "pickle.dump(nyc_311, nyc_311_pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn nyc 311 into a geodataframe\n",
    "nyc_311 = pickle.load(open(f\"{data_dir}{sep}pickle{sep}nyc_311.pickle\", \"rb\"))\n",
    "nyc_311_gdf = gp.GeoDataFrame(nyc_311, geometry=gp.points_from_xy(nyc_311.longitude, nyc_311.latitude))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round_lat_long(nyc_311_gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run through the assign ct function\n",
    "nyc_311_gdf = assign_ct(nyc_311_gdf, 'nyc_311_gdf', count_hundred=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collapse 311 count by CT & year\n",
    "nyc_311_collapsed = nyc_311_gdf[['descriptor','LocationCT', 'year']].groupby([\"LocationCT\", \"year\"]) \\\n",
    "        .agg('count').rename(columns={'descriptor': 'count'}).reset_index(level=['LocationCT', 'year'])\n",
    "\n",
    "# Merge geometries back in using projected shapefile. Use a right merge to ensure all census tracts are \n",
    "# included (b/c not all census tracts are in the collapsed count for every year)\n",
    "to_merge = nycct_proj[['geometry', 'fips_code']].rename(columns={'fips_code': 'LocationCT'})\n",
    "collapsed_311_count = gp.GeoDataFrame(nyc_311_collapsed.merge(to_merge, how='right', on='LocationCT'))\n",
    "collapsed_311_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collapsed_311_count['count'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create heat maps using create_heat_map function from earlier\n",
    "for year in [2010,2011,2012,2013]:\n",
    "    year_str = str(year)\n",
    "    create_heat_map(collapsed_311_count, year_str, f'Count of 311 Calls by Census Tract in {year_str}',\n",
    "                   f'311_calls_ct_{year_str}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### crime data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull in NYC historical crime data (also uses Socrata data)\n",
    "select_string = 'cmplnt_num, cmplnt_fr_dt AS date, date_extract_y(cmplnt_fr_dt) AS year,' \\\n",
    "    'date_extract_m(cmplnt_fr_dt) AS month,  pd_cd AS class, pd_desc, law_cat_cd AS level, crm_atpt_cptd_cd AS completed, latitude, longitude'\n",
    "where_string = 'latitude IS NOT NULL AND (year = 2006 OR year = 2007 OR year = 2008 OR year = 2009 OR year = 2010 OR year = 2011 OR year = 2012 OR year = 2013)'\n",
    "nyc_crime = socrata_API_df(source_domain = \"data.cityofnewyork.us\", dataset_id = 'qgea-i56i', \\\n",
    "                           select_string = select_string, where_string = where_string, limit = 4000000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle crime dataset\n",
    "# pickle.dump(nyc_crime, open(f\"{data_dir}{sep}pickle{sep}nyc_crime.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_crime = pickle.load(open(f\"{data_dir}{sep}pickle{sep}nyc_crime.pickle\", \"rb\"))\n",
    "nyc_crime_gdf = gp.GeoDataFrame(nyc_crime, geometry=gp.points_from_xy(nyc_crime.longitude, nyc_crime.latitude))\n",
    "\n",
    "# Cut lat/long to 6 digits\n",
    "round_lat_long(nyc_crime_gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign CT\n",
    "nyc_crime_gdf = assign_ct(nyc_crime_gdf, 'nyc_crime_gdf', count_hundred=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collapse crime count by CT & year\n",
    "nyc_crime_collapsed = nyc_crime_gdf[['cmplnt_num','LocationCT', 'year']].groupby([\"LocationCT\", \"year\"]) \\\n",
    "        .agg('count').rename(columns={'cmplnt_num': 'count'}).reset_index(level=['LocationCT', 'year'])\n",
    "\n",
    "# Merge geometries back in using projected shapefile. \n",
    "to_merge = nycct_proj[['geometry', 'fips_code']].rename(columns={'fips_code': 'LocationCT'})\n",
    "collapsed_crime_count = gp.GeoDataFrame(nyc_crime_collapsed.merge(to_merge, on='LocationCT'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make heat maps\n",
    "for year in [2010,2011,2012,2013]:\n",
    "    year_str = str(year)\n",
    "    create_heat_map(collapsed_crime_count, year_str, f'Crime Count by Census Tract in {year_str}',\n",
    "                   f'total_crime_ct_{year_str}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### HUD dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring in HUD vacant addresses data\n",
    "# Create list of the excel files that will need to be loaded in \n",
    "# Glob.glob creates a list of all the files that end in .xlsx in the directory of HUD vacant data\n",
    "# The rest of the command filters out jsons that end in 00000.json since those represent meta counts and not actual tweets\n",
    "hud_list = [j for j in glob.glob(f'{data_dir}{sep}hud_vacant_data{sep}*.csv')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hud_df = pd.DataFrame()\n",
    "for file in hud_list:\n",
    "    temp_file = pd.read_csv(file, sep = None, engine='python')\n",
    "#   Using title of the file, create a column for the year & month/quarter\n",
    "    temp_file['year'] = [\"20\"+file[32:34] for i in range(temp_file.shape[0])]\n",
    "    temp_file['month'] = [file[28:30] for i in range(temp_file.shape[0])]\n",
    "    hud_df = hud_df.append(temp_file).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a FIPS code variable that is equal to the string of geoid \n",
    "# (note that b/c it's in integers, we need to re-add leading zero for states with fips codes < 10) \n",
    "hud_df['fips_code'] = hud_df[\"GEOID\"].apply(lambda x: (\"0\" + str(x))[-11:])\n",
    "\n",
    "# Create file with only NY \n",
    "ny_hud = hud_df[hud_df['fips_code'].apply(lambda x: x[0:2] == \"36\")].reset_index(drop=True) \n",
    "\n",
    "# Pickle ny hug file\n",
    "nyc_hud_pickle = open(f\"{data_dir}{sep}pickle{sep}nyc_hud.pickle\", \"wb\")\n",
    "pickle.dump(ny_hud, nyc_hud_pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpickle ny hud\n",
    "ny_hud = pickle.load(open(f\"{data_dir}{sep}pickle{sep}nyc_hud.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Businesses "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring in NYC businesses data. Some businesses have lat/long but not census tract and some vice versa\n",
    "select_string = 'license_nbr, license_type, license_status, date_extract_y(license_creation_date) as year, date_extract_m(license_creation_date) as month,' \\\n",
    "                'industry, business_name, longitude, latitude'\n",
    "where_string = 'latitude IS NOT NULL AND (year = 2011 OR year = 2012 OR year = 2013)'\n",
    "nyc_bus = socrata_API_df(\"data.cityofnewyork.us\", \"w7w3-xahh\", select_string = select_string, \n",
    "               where_string = where_string, limit = 50000)\n",
    "\n",
    "pickle.dump(nyc_bus, open(f\"{data_dir}{sep}pickle{sep}nyc_businesses.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpickle \n",
    "nyc_bus = pickle.load(open(f\"{data_dir}{sep}pickle{sep}nyc_businesses.pickle\", \"rb\"))\n",
    "\n",
    "nyc_bus_gdf = gp.GeoDataFrame(nyc_bus, geometry=gp.points_from_xy(nyc_bus.longitude, nyc_bus.latitude))\n",
    "# Cut lat/long to 6 digits\n",
    "round_lat_long(nyc_bus_gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_bus_gdf = assign_ct(nyc_bus_gdf, 'nyc_bus_gdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
